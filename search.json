[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fictional",
    "section": "",
    "text": "news\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-06-19-health_data.html",
    "href": "posts/2021-06-19-health_data.html",
    "title": "Forecasting running data",
    "section": "",
    "text": "from datetime import date\nimport os \ntoday = date.today()\n\n\nfor file in os.listdir():\n    if file.endswith('.ipynb'):\n        cd=today\n        os.rename(file, f'{today}-{file}')\n\n\nimport shutil\n\nshutil.copy(\n    os.path.join('2021-06-19-health_data.ipynb'),\n    os.path.join('../git-repos/Kearney_Data_Science/_notebooks')\n)\n\n'../git-repos/Kearney_Data_Science/_notebooks/2021-06-19-health_data.ipynb'\n\n\n\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\nengine = db.create_engine('sqlite:///../../Downloads/fitbit.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\n\nsql = \"\"\"\nselect DATE(date_time) as day\n, sum(distance_miles) as distance\nfrom distance_v\ngroup by DATE(date_time)\n\"\"\"\n\ncnxn = connection\n\ndf = pd.read_sql(sql, cnxn)\n\ndf\n\n\n\n\n\n  \n    \n      \n      day\n      distance\n    \n  \n  \n    \n      0\n      2020-12-02\n      11.238989\n    \n    \n      1\n      2020-12-03\n      7.615898\n    \n    \n      2\n      2020-12-04\n      11.392033\n    \n    \n      3\n      2020-12-05\n      9.929077\n    \n    \n      4\n      2020-12-06\n      10.442889\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      186\n      2021-06-08\n      0.935723\n    \n    \n      187\n      2021-06-09\n      4.844334\n    \n    \n      188\n      2021-06-10\n      8.554417\n    \n    \n      189\n      2021-06-11\n      6.167171\n    \n    \n      190\n      2021-06-12\n      5.006263\n    \n  \n\n191 rows × 2 columns\n\n\n\n\ndf['ds'] = df.day\ndf['y'] = df.distance\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 191 entries, 0 to 190\nData columns (total 4 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   day       191 non-null    object \n 1   distance  191 non-null    float64\n 2   ds        191 non-null    object \n 3   y         191 non-null    float64\ndtypes: float64(2), object(2)\nmemory usage: 6.1+ KB\n\n\n\nimport statsmodels.api as sm\nimport pandas as pd\nfrom prophet import Prophet\n\n\nimport pandas as pd\npd.set_option('compute.use_numexpr', False)\n\nm = Prophet()\nm.fit(df)\n\nINFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n<prophet.forecaster.Prophet at 0x7f0100bb6c70>\n\n\n\nfuture = m.make_future_dataframe(periods=365)\nfuture.tail()\n\n\n\n\n\n  \n    \n      \n      ds\n    \n  \n  \n    \n      551\n      2022-06-08\n    \n    \n      552\n      2022-06-09\n    \n    \n      553\n      2022-06-10\n    \n    \n      554\n      2022-06-11\n    \n    \n      555\n      2022-06-12\n    \n  \n\n\n\n\n\nforecast = m.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n\n\n\n\n\n  \n    \n      \n      ds\n      yhat\n      yhat_lower\n      yhat_upper\n    \n  \n  \n    \n      551\n      2022-06-08\n      9.954126\n      5.626830\n      14.090458\n    \n    \n      552\n      2022-06-09\n      11.067850\n      6.415623\n      15.269029\n    \n    \n      553\n      2022-06-10\n      9.524963\n      5.164365\n      13.816646\n    \n    \n      554\n      2022-06-11\n      10.393233\n      5.943450\n      14.634128\n    \n    \n      555\n      2022-06-12\n      11.320983\n      6.836854\n      15.647303\n    \n  \n\n\n\n\n\nfig1 = m.plot(forecast)\n\n\n\n\n\nfig2 = m.plot_components(forecast)\n\n\n\n\n\n# Python\nfig1 = m.plot(forecast)\n\n\n\n\n\n# Python\nfig2 = m.plot_components(forecast)\n\n\n\n\n\n# Python\nfrom prophet.plot import plot_plotly, plot_components_plotly\n\nplot_plotly(m, forecast)\n\n\n                                                \n\n\n\n# Python\nplot_components_plotly(m, forecast)\n\n\n                                                \n\n\n\n# Model fit\nm = Prophet() #Instanticate from Prophet class. \nm.fit(df) # Fit the Prophet model.\n\n# Predict\nfuture = m.make_future_dataframe(periods=365) # Make future date data frame for the next 365 days (it gives daily because it follows the frequency in input dataframe by default).\nforecast = m.predict(future) # Predict future value.\n\n# Plot results\nfig1 = m.plot(forecast) # Plot the fit to past data and future forcast.\nfig2 = m.plot_components(forecast) # Plot breakdown of components.\nplt.show()\nforecast # Displaying various results in table format.\n\nINFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      ds\n      trend\n      yhat_lower\n      yhat_upper\n      trend_lower\n      trend_upper\n      additive_terms\n      additive_terms_lower\n      additive_terms_upper\n      weekly\n      weekly_lower\n      weekly_upper\n      multiplicative_terms\n      multiplicative_terms_lower\n      multiplicative_terms_upper\n      yhat\n    \n  \n  \n    \n      0\n      2020-12-02\n      8.826908\n      4.690634\n      11.818085\n      8.826908\n      8.826908\n      -0.380921\n      -0.380921\n      -0.380921\n      -0.380921\n      -0.380921\n      -0.380921\n      0.0\n      0.0\n      0.0\n      8.445987\n    \n    \n      1\n      2020-12-03\n      8.805304\n      5.826039\n      12.902548\n      8.805304\n      8.805304\n      0.727109\n      0.727109\n      0.727109\n      0.727109\n      0.727109\n      0.727109\n      0.0\n      0.0\n      0.0\n      9.532413\n    \n    \n      2\n      2020-12-04\n      8.783700\n      4.373529\n      11.382860\n      8.783700\n      8.783700\n      -0.821473\n      -0.821473\n      -0.821473\n      -0.821473\n      -0.821473\n      -0.821473\n      0.0\n      0.0\n      0.0\n      7.962227\n    \n    \n      3\n      2020-12-05\n      8.762096\n      5.593958\n      12.411443\n      8.762096\n      8.762096\n      0.041102\n      0.041102\n      0.041102\n      0.041102\n      0.041102\n      0.041102\n      0.0\n      0.0\n      0.0\n      8.803198\n    \n    \n      4\n      2020-12-06\n      8.740492\n      6.342656\n      13.346898\n      8.740492\n      8.740492\n      0.963158\n      0.963158\n      0.963158\n      0.963158\n      0.963158\n      0.963158\n      0.0\n      0.0\n      0.0\n      9.703650\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      551\n      2022-06-08\n      10.335047\n      5.742947\n      14.393370\n      7.579529\n      13.222686\n      -0.380921\n      -0.380921\n      -0.380921\n      -0.380921\n      -0.380921\n      -0.380921\n      0.0\n      0.0\n      0.0\n      9.954126\n    \n    \n      552\n      2022-06-09\n      10.340742\n      6.769705\n      15.861835\n      7.575976\n      13.239863\n      0.727109\n      0.727109\n      0.727109\n      0.727109\n      0.727109\n      0.727109\n      0.0\n      0.0\n      0.0\n      11.067850\n    \n    \n      553\n      2022-06-10\n      10.346436\n      4.792225\n      14.159937\n      7.573519\n      13.261637\n      -0.821473\n      -0.821473\n      -0.821473\n      -0.821473\n      -0.821473\n      -0.821473\n      0.0\n      0.0\n      0.0\n      9.524963\n    \n    \n      554\n      2022-06-11\n      10.352131\n      5.946817\n      14.833565\n      7.570061\n      13.285497\n      0.041102\n      0.041102\n      0.041102\n      0.041102\n      0.041102\n      0.041102\n      0.0\n      0.0\n      0.0\n      10.393233\n    \n    \n      555\n      2022-06-12\n      10.357825\n      7.099368\n      15.701414\n      7.563656\n      13.302798\n      0.963158\n      0.963158\n      0.963158\n      0.963158\n      0.963158\n      0.963158\n      0.0\n      0.0\n      0.0\n      11.320983\n    \n  \n\n556 rows × 16 columns\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Load test data: log-transformed daily page views for the Wikipedia page for Peyton Manning.\n\ndf['cap'] = 10 # Saturating maximum\ndf['floor'] = 7 # Saturating minimum\n\n# Model setup\nm = Prophet(growth='logistic')\nm.add_country_holidays(country_name='US') # Adding US holiday regressor\nm.fit(df) \n\n# Future data generation\nfuture = m.make_future_dataframe(periods=365*5)\nfuture['cap'] = 10 # Saturating maximum\nfuture['floor'] = 7 # Saturating minimum\n\n# Future forecast\nforecast = m.predict(future) \n\nINFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n\n\n# Visualize\nfig1 = m.plot(forecast) # Plot the fit to past data and future forcast.\nfig2 = m.plot_components(forecast) # Plot breakdown of components.\nplt.show()\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef is_nfl_season(ds):\n    date = pd.to_datetime(ds)\n    return (date.month > 8 or date.month < 2)\n\ndf['on_season'] = df['ds'].apply(is_nfl_season) #on_season dummy.\ndf['off_season'] = ~df['ds'].apply(is_nfl_season) #off_season dummy.\n\n# set user-defined seasonality and fit\nm = Prophet(weekly_seasonality=False)\nm.add_seasonality(name='weekly_on_season', period=7, fourier_order=3, condition_name='on_season')\nm.add_seasonality(name='weekly_off_season', period=7, fourier_order=3, condition_name='off_season')\nm.fit(df)\n\n# Make the same columns to future data.\nfuture = m.make_future_dataframe(periods=365*5) # Make future date data frame for the next 365 days (it gives daily because it follows the frequency in input dataframe by default).\nfuture['on_season'] = future['ds'].apply(is_nfl_season)\nfuture['off_season'] = ~future['ds'].apply(is_nfl_season)\n\n# Predict future value.\nforecast = m.predict(future)\n\nINFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n\n# Plot results\nfig1 = m.plot(forecast) # Plot the fit to past data and future forcast.\nfig2 = m.plot_components(forecast) # Plot breakdown of components.\nplt.show()\n\n\n\n\n\n\n\n\n# After getting forecast dataframe using user-defined seasonality \"on-season\"/\"off-season\" above...\n\nfrom statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n\ndf['ds'] = pd.to_datetime(df['ds'],format='%Y-%m-%d')\ndf_res = df.merge(forecast,how=\"inner\",on=\"ds\")\ndf_res['residual'] = df_res['y'] - df_res['yhat']\nplot_acf(df_res['residual'])\nplot_pacf(df_res['residual'])\nplt.show()"
  },
  {
    "objectID": "posts/2021-06-10-regression-pycaret-2.html",
    "href": "posts/2021-06-10-regression-pycaret-2.html",
    "title": "Regression using Fiscal Data with PyCaret",
    "section": "",
    "text": "# %load solutions/regression_example.py\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\n\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\n\n\nX, y = df.drop(['specific', 'Unnamed: 0'], axis = 1), df['specific']\n\n\nX = X.select_dtypes(include='number')\nX\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\n\ndf=df.drop(['Unnamed: 0'], axis = 1)\n\n\n\n\n\nfrom pycaret.regression import *\nreg1 = setup(df, target = 'specific', session_id=153, log_experiment=True, experiment_name='fiscal')\n\n\n                    Description        Value    \n                \n                        0\n                        session_id\n                        153\n            \n            \n                        1\n                        Target\n                        specific\n            \n            \n                        2\n                        Original Data\n                        (118, 12)\n            \n            \n                        3\n                        Missing Values\n                        False\n            \n            \n                        4\n                        Numeric Features\n                        8\n            \n            \n                        5\n                        Categorical Features\n                        3\n            \n            \n                        6\n                        Ordinal Features\n                        False\n            \n            \n                        7\n                        High Cardinality Features\n                        False\n            \n            \n                        8\n                        High Cardinality Method\n                        None\n            \n            \n                        9\n                        Transformed Train Set\n                        (82, 47)\n            \n            \n                        10\n                        Transformed Test Set\n                        (36, 47)\n            \n            \n                        11\n                        Shuffle Train-Test\n                        True\n            \n            \n                        12\n                        Stratify Train-Test\n                        False\n            \n            \n                        13\n                        Fold Generator\n                        KFold\n            \n            \n                        14\n                        Fold Number\n                        10\n            \n            \n                        15\n                        CPU Jobs\n                        -1\n            \n            \n                        16\n                        Use GPU\n                        False\n            \n            \n                        17\n                        Log Experiment\n                        True\n            \n            \n                        18\n                        Experiment Name\n                        fiscal\n            \n            \n                        19\n                        USI\n                        0884\n            \n            \n                        20\n                        Imputation Type\n                        simple\n            \n            \n                        21\n                        Iterative Imputation Iteration\n                        None\n            \n            \n                        22\n                        Numeric Imputer\n                        mean\n            \n            \n                        23\n                        Iterative Imputation Numeric Model\n                        None\n            \n            \n                        24\n                        Categorical Imputer\n                        constant\n            \n            \n                        25\n                        Iterative Imputation Categorical Model\n                        None\n            \n            \n                        26\n                        Unknown Categoricals Handling\n                        least_frequent\n            \n            \n                        27\n                        Normalize\n                        False\n            \n            \n                        28\n                        Normalize Method\n                        None\n            \n            \n                        29\n                        Transformation\n                        False\n            \n            \n                        30\n                        Transformation Method\n                        None\n            \n            \n                        31\n                        PCA\n                        False\n            \n            \n                        32\n                        PCA Method\n                        None\n            \n            \n                        33\n                        PCA Components\n                        None\n            \n            \n                        34\n                        Ignore Low Variance\n                        False\n            \n            \n                        35\n                        Combine Rare Levels\n                        False\n            \n            \n                        36\n                        Rare Level Threshold\n                        None\n            \n            \n                        37\n                        Numeric Binning\n                        False\n            \n            \n                        38\n                        Remove Outliers\n                        False\n            \n            \n                        39\n                        Outliers Threshold\n                        None\n            \n            \n                        40\n                        Remove Multicollinearity\n                        False\n            \n            \n                        41\n                        Multicollinearity Threshold\n                        None\n            \n            \n                        42\n                        Clustering\n                        False\n            \n            \n                        43\n                        Clustering Iteration\n                        None\n            \n            \n                        44\n                        Polynomial Features\n                        False\n            \n            \n                        45\n                        Polynomial Degree\n                        None\n            \n            \n                        46\n                        Trignometry Features\n                        False\n            \n            \n                        47\n                        Polynomial Threshold\n                        None\n            \n            \n                        48\n                        Group Features\n                        False\n            \n            \n                        49\n                        Feature Selection\n                        False\n            \n            \n                        50\n                        Feature Selection Method\n                        classic\n            \n            \n                        51\n                        Features Selection Threshold\n                        None\n            \n            \n                        52\n                        Feature Interaction\n                        False\n            \n            \n                        53\n                        Feature Ratio\n                        False\n            \n            \n                        54\n                        Interaction Threshold\n                        None\n            \n            \n                        55\n                        Transform Target\n                        False\n            \n            \n                        56\n                        Transform Target Method\n                        box-cox\n            \n    \n\n\n\nbest_model = compare_models(fold=5)\n\n\n                    Model        MAE        MSE        RMSE        R2        RMSLE        MAPE        TT (Sec)    \n                \n                        ridge\n                        Ridge Regression\n                        203104.6812\n                        70889717760.0000\n                        264074.3844\n                        0.8726\n                        0.5540\n                        0.3956\n                        0.0080\n            \n            \n                        en\n                        Elastic Net\n                        214984.3531\n                        85422610841.6000\n                        290426.9344\n                        0.8517\n                        0.4986\n                        0.4002\n                        0.0100\n            \n            \n                        br\n                        Bayesian Ridge\n                        220166.1782\n                        95589994393.9887\n                        304401.6991\n                        0.8301\n                        0.4481\n                        0.3928\n                        0.0120\n            \n            \n                        huber\n                        Huber Regressor\n                        220856.7956\n                        112309915361.3680\n                        329346.7142\n                        0.8236\n                        0.4063\n                        0.3861\n                        0.0240\n            \n            \n                        lr\n                        Linear Regression\n                        232120.2812\n                        103810244608.0000\n                        317848.8281\n                        0.8138\n                        0.5000\n                        0.4204\n                        0.4660\n            \n            \n                        et\n                        Extra Trees Regressor\n                        221900.6764\n                        127140774212.6801\n                        338961.7060\n                        0.7945\n                        0.3815\n                        0.3499\n                        0.0580\n            \n            \n                        rf\n                        Random Forest Regressor\n                        237185.4749\n                        140134962286.5481\n                        359066.7448\n                        0.7677\n                        0.3845\n                        0.3603\n                        0.0680\n            \n            \n                        gbr\n                        Gradient Boosting Regressor\n                        238720.3298\n                        145838870195.5470\n                        366741.3828\n                        0.7624\n                        0.3810\n                        0.3619\n                        0.0200\n            \n            \n                        knn\n                        K Neighbors Regressor\n                        285577.7062\n                        149621195571.2000\n                        378386.5938\n                        0.7535\n                        0.4782\n                        0.4564\n                        0.0080\n            \n            \n                        omp\n                        Orthogonal Matching Pursuit\n                        238278.1124\n                        126779634746.9780\n                        340431.6364\n                        0.7507\n                        0.6785\n                        0.4087\n                        0.0060\n            \n            \n                        ada\n                        AdaBoost Regressor\n                        286133.9032\n                        178448925624.8169\n                        409351.8897\n                        0.7261\n                        0.4671\n                        0.4826\n                        0.0380\n            \n            \n                        par\n                        Passive Aggressive Regressor\n                        333654.6862\n                        255709611689.0604\n                        478365.7421\n                        0.6396\n                        0.6616\n                        0.4911\n                        0.0080\n            \n            \n                        lightgbm\n                        Light Gradient Boosting Machine\n                        333751.2407\n                        246645596801.5230\n                        489542.3762\n                        0.6196\n                        0.4881\n                        0.4594\n                        0.0140\n            \n            \n                        dt\n                        Decision Tree Regressor\n                        331466.4338\n                        251572931731.8265\n                        484401.1935\n                        0.5996\n                        0.4895\n                        0.4942\n                        0.0080\n            \n            \n                        lasso\n                        Lasso Regression\n                        472806.4594\n                        1744652831948.8000\n                        924353.6562\n                        -2.9647\n                        0.9793\n                        0.7323\n                        0.3020\n            \n            \n                        llar\n                        Lasso Least Angle Regression\n                        557614.3428\n                        2757565135711.8994\n                        1218269.0934\n                        -4.1517\n                        0.9882\n                        0.9808\n                        0.0120\n            \n            \n                        lar\n                        Least Angle Regression\n                        523505032166121.1875\n                        8777827809541126967434359603200.0000\n                        1651376809056778.0000\n                        -21875898822041108480.0000\n                        12.5860\n                        2953087708.0914\n                        0.0120\n            \n    \n\n\n\ngbr = create_model('gbr')\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        259539.4234\n                        188010814764.0497\n                        433602.1388\n                        0.2035\n                        0.3725\n                        0.3658\n            \n            \n                        1\n                        344439.6555\n                        261157479760.8762\n                        511035.6932\n                        0.7712\n                        0.2734\n                        0.2021\n            \n            \n                        2\n                        269448.7502\n                        89799376760.5959\n                        299665.4414\n                        0.2559\n                        0.5134\n                        0.5621\n            \n            \n                        3\n                        156389.8428\n                        56101010341.3762\n                        236856.5185\n                        0.9215\n                        0.3037\n                        0.2752\n            \n            \n                        4\n                        197734.1876\n                        68770895511.2340\n                        262242.0552\n                        0.8442\n                        0.4341\n                        0.4405\n            \n            \n                        5\n                        316382.5762\n                        190021156955.1915\n                        435914.1624\n                        0.8431\n                        0.3250\n                        0.3036\n            \n            \n                        6\n                        132877.7936\n                        48011457619.3648\n                        219115.1698\n                        0.9377\n                        0.1445\n                        0.1184\n            \n            \n                        7\n                        63780.4855\n                        6638335948.5179\n                        81475.9839\n                        0.9926\n                        0.2004\n                        0.1484\n            \n            \n                        8\n                        84622.6556\n                        19489890756.1842\n                        139606.1988\n                        0.8903\n                        0.5672\n                        0.5448\n            \n            \n                        9\n                        312499.9655\n                        219320548133.8201\n                        468316.7178\n                        0.6284\n                        0.4557\n                        0.4542\n            \n            \n                        Mean\n                        213771.5336\n                        114732096655.1211\n                        308783.0080\n                        0.7288\n                        0.3590\n                        0.3415\n            \n            \n                        SD\n                        95820.4252\n                        86484686219.4249\n                        139230.5665\n                        0.2673\n                        0.1285\n                        0.1501\n            \n    \n\n\n\nimport numpy as np\ngbrs = [create_model('gbr', learning_rate=i) for i in np.arange(0.1,1,0.1)]\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        214161.7167\n                        66589303052.3318\n                        258049.0323\n                        0.7179\n                        0.5064\n                        0.3315\n            \n            \n                        1\n                        383023.4934\n                        211259341300.2875\n                        459629.5697\n                        0.8149\n                        0.3382\n                        0.2795\n            \n            \n                        2\n                        239279.0742\n                        87706228150.9347\n                        296152.3732\n                        0.2732\n                        0.5829\n                        0.5918\n            \n            \n                        3\n                        192434.2246\n                        95811714139.4744\n                        309534.6736\n                        0.8660\n                        0.4334\n                        0.4130\n            \n            \n                        4\n                        142428.6249\n                        51054538060.0787\n                        225952.5128\n                        0.8843\n                        0.2410\n                        0.1901\n            \n            \n                        5\n                        367843.9369\n                        206432485479.9056\n                        454348.4186\n                        0.8295\n                        0.5008\n                        0.4725\n            \n            \n                        6\n                        228995.4083\n                        126313136112.6237\n                        355405.5938\n                        0.8362\n                        0.2683\n                        0.2024\n            \n            \n                        7\n                        158840.0937\n                        40230541391.6350\n                        200575.5254\n                        0.9550\n                        0.4181\n                        0.2684\n            \n            \n                        8\n                        195178.1770\n                        46865758291.9661\n                        216485.0071\n                        0.7363\n                        0.5635\n                        0.6440\n            \n            \n                        9\n                        371923.4898\n                        271120545391.5568\n                        520692.3712\n                        0.5406\n                        0.5683\n                        0.5885\n            \n            \n                        Mean\n                        249410.8239\n                        120338359137.0794\n                        329682.5078\n                        0.7454\n                        0.4421\n                        0.3982\n            \n            \n                        SD\n                        86305.1580\n                        77214568547.1943\n                        107924.9888\n                        0.1907\n                        0.1184\n                        0.1603\n            \n    \n\n\n\nprint(len(gbrs))\n\n9\n\n\n\ntuned_gbr = tune_model(gbr, n_iter=50, optimize = 'RMSE')\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        154942.6734\n                        74997708663.9170\n                        273857.0953\n                        0.6823\n                        0.2835\n                        0.2430\n            \n            \n                        1\n                        356026.5185\n                        259197991105.0069\n                        509114.9095\n                        0.7729\n                        0.3057\n                        0.2048\n            \n            \n                        2\n                        196245.5979\n                        50062838730.3433\n                        223747.2653\n                        0.5852\n                        0.4460\n                        0.4092\n            \n            \n                        3\n                        130813.1472\n                        28724978265.9940\n                        169484.4484\n                        0.9598\n                        0.3879\n                        0.3551\n            \n            \n                        4\n                        150379.1478\n                        58016543407.9028\n                        240866.2355\n                        0.8686\n                        0.3318\n                        0.2935\n            \n            \n                        5\n                        360842.5596\n                        256418976969.6155\n                        506378.2943\n                        0.7883\n                        0.3802\n                        0.3776\n            \n            \n                        6\n                        101664.5162\n                        23660563887.4224\n                        153819.9073\n                        0.9693\n                        0.1738\n                        0.1305\n            \n            \n                        7\n                        121078.1094\n                        22126447635.5874\n                        148749.6139\n                        0.9753\n                        0.3012\n                        0.2768\n            \n            \n                        8\n                        204892.4698\n                        59961129586.8548\n                        244869.6175\n                        0.6626\n                        0.7065\n                        0.9038\n            \n            \n                        9\n                        247798.4343\n                        119125737964.7928\n                        345145.9662\n                        0.7982\n                        0.3572\n                        0.3649\n            \n            \n                        Mean\n                        202468.3174\n                        95229291621.7437\n                        281603.3353\n                        0.8062\n                        0.3674\n                        0.3559\n            \n            \n                        SD\n                        88121.5888\n                        85677172457.2040\n                        126209.5604\n                        0.1300\n                        0.1326\n                        0.2000\n            \n    \n\n\n\ntuned_gbr\n\nGradientBoostingRegressorGradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n                          init=None, learning_rate=0.05, loss='ls', max_depth=8,\n                          max_features='sqrt', max_leaf_nodes=None,\n                          min_impurity_decrease=0.001, min_impurity_split=None,\n                          min_samples_leaf=3, min_samples_split=10,\n                          min_weight_fraction_leaf=0.0, n_estimators=260,\n                          n_iter_no_change=None, presort='deprecated',\n                          random_state=153, subsample=1.0, tol=0.0001,\n                          validation_fraction=0.1, verbose=0, warm_start=False)\n\n\n\ndt = create_model('dt')\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        207956.4444\n                        64209396573.1111\n                        253395.7312\n                        0.7280\n                        0.2948\n                        0.3014\n            \n            \n                        1\n                        524236.3333\n                        569991781768.5555\n                        754978.0009\n                        0.5006\n                        0.4063\n                        0.3254\n            \n            \n                        2\n                        329215.7500\n                        190393519675.5000\n                        436341.0589\n                        -0.5777\n                        0.5801\n                        0.6555\n            \n            \n                        3\n                        191451.2500\n                        124860637323.7500\n                        353356.2470\n                        0.8254\n                        0.4116\n                        0.3334\n            \n            \n                        4\n                        213423.5000\n                        76306439743.2500\n                        276236.2028\n                        0.8271\n                        0.5254\n                        0.3705\n            \n            \n                        5\n                        370690.3750\n                        199965742624.8750\n                        447175.2929\n                        0.8349\n                        0.5442\n                        0.4049\n            \n            \n                        6\n                        246669.8750\n                        129638363043.8750\n                        360053.2781\n                        0.8319\n                        0.2617\n                        0.2171\n            \n            \n                        7\n                        178672.0000\n                        77945893211.5000\n                        279187.9174\n                        0.9128\n                        0.3495\n                        0.3173\n            \n            \n                        8\n                        166587.8750\n                        46127063745.6250\n                        214772.1205\n                        0.7404\n                        0.4736\n                        0.4654\n            \n            \n                        9\n                        295026.8750\n                        185238337215.6250\n                        430393.2356\n                        0.6861\n                        0.3864\n                        0.3666\n            \n            \n                        Mean\n                        272393.0278\n                        166467717492.5666\n                        380588.9085\n                        0.6310\n                        0.4234\n                        0.3757\n            \n            \n                        SD\n                        105664.3855\n                        144523328671.4254\n                        147036.7308\n                        0.4171\n                        0.1010\n                        0.1121\n            \n    \n\n\n\nbagged_dt = ensemble_model(dt, n_estimators=50)\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        196727.1578\n                        90285018684.0473\n                        300474.6556\n                        0.6175\n                        0.3087\n                        0.2890\n            \n            \n                        1\n                        400549.3422\n                        340151201313.8998\n                        583224.8291\n                        0.7020\n                        0.3095\n                        0.2039\n            \n            \n                        2\n                        225912.9050\n                        72423782552.9881\n                        269116.6709\n                        0.3999\n                        0.4933\n                        0.5057\n            \n            \n                        3\n                        118783.1250\n                        22526053317.3862\n                        150086.8193\n                        0.9685\n                        0.3121\n                        0.2808\n            \n            \n                        4\n                        202532.9275\n                        80967074782.4978\n                        284547.1398\n                        0.8166\n                        0.4177\n                        0.4139\n            \n            \n                        5\n                        341289.0375\n                        234909197221.4275\n                        484674.3208\n                        0.8060\n                        0.3879\n                        0.3623\n            \n            \n                        6\n                        141661.2425\n                        37608256142.0725\n                        193928.4820\n                        0.9512\n                        0.1464\n                        0.1355\n            \n            \n                        7\n                        126158.8400\n                        34803232314.1118\n                        186556.2444\n                        0.9611\n                        0.3018\n                        0.2442\n            \n            \n                        8\n                        183361.3550\n                        44385100050.7344\n                        210677.7161\n                        0.7502\n                        0.5438\n                        0.5929\n            \n            \n                        9\n                        260316.8175\n                        176470763137.0123\n                        420084.2334\n                        0.7010\n                        0.3999\n                        0.3821\n            \n            \n                        Mean\n                        219729.2750\n                        113452967951.6178\n                        308337.1111\n                        0.7674\n                        0.3621\n                        0.3410\n            \n            \n                        SD\n                        87376.1209\n                        99179989784.9265\n                        135577.2615\n                        0.1680\n                        0.1066\n                        0.1322\n            \n    \n\n\n\nboosted_dt = ensemble_model(dt, method = 'Boosting')\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        261162.8889\n                        114911582041.3333\n                        338986.1089\n                        0.5132\n                        0.3872\n                        0.3925\n            \n            \n                        1\n                        422328.3333\n                        333782428795.6667\n                        577739.0664\n                        0.7076\n                        0.3211\n                        0.2542\n            \n            \n                        2\n                        232284.1250\n                        77562868468.1250\n                        278501.1104\n                        0.3573\n                        0.5087\n                        0.5015\n            \n            \n                        3\n                        197047.1250\n                        112221331803.8750\n                        334994.5250\n                        0.8431\n                        0.3502\n                        0.3395\n            \n            \n                        4\n                        285119.7500\n                        161644974606.0000\n                        402050.9602\n                        0.6338\n                        0.5596\n                        0.5738\n            \n            \n                        5\n                        473330.2500\n                        599114250508.0000\n                        774024.7092\n                        0.5053\n                        0.5119\n                        0.5217\n            \n            \n                        6\n                        108483.0000\n                        20435489036.7500\n                        142952.7511\n                        0.9735\n                        0.2098\n                        0.1500\n            \n            \n                        7\n                        157960.0000\n                        69455073830.5000\n                        263543.3054\n                        0.9223\n                        0.2735\n                        0.2394\n            \n            \n                        8\n                        120478.7500\n                        23354347455.5000\n                        152821.2925\n                        0.8686\n                        0.5655\n                        0.5893\n            \n            \n                        9\n                        231595.7500\n                        117602419885.0000\n                        342932.0922\n                        0.8007\n                        0.4401\n                        0.4379\n            \n            \n                        Mean\n                        248978.9972\n                        163008476643.0750\n                        360854.5921\n                        0.7125\n                        0.4128\n                        0.4000\n            \n            \n                        SD\n                        113864.7400\n                        167985632403.4423\n                        181086.8299\n                        0.1940\n                        0.1176\n                        0.1435\n            \n    \n\n\n\nplot_model(dt)\n\n\n\n\n\nplot_model(dt, plot = 'error')\n\n\n\n\n\nplot_model(dt, plot = 'feature')\n\n\n\n\n\nevaluate_model(dt)\n\n\n\n\n\n  \n    \n      \n      Parameters\n    \n  \n  \n    \n      ccp_alpha\n      0.0\n    \n    \n      criterion\n      mse\n    \n    \n      max_depth\n      None\n    \n    \n      max_features\n      None\n    \n    \n      max_leaf_nodes\n      None\n    \n    \n      min_impurity_decrease\n      0.0\n    \n    \n      min_impurity_split\n      None\n    \n    \n      min_samples_leaf\n      1\n    \n    \n      min_samples_split\n      2\n    \n    \n      min_weight_fraction_leaf\n      0.0\n    \n    \n      presort\n      deprecated\n    \n    \n      random_state\n      153\n    \n    \n      splitter\n      best\n    \n  \n\n\n\n\n\ninterpret_model(dt)\n\n\n\n\n\ninterpret_model(dt, plot = 'correlation')\n\n\n\n\n\ninterpret_model(dt, plot = 'reason', observation = 12)\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\nbest = automl(optimize = 'MAE')\nbest\n\nGradientBoostingRegressorGradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n                          init=None, learning_rate=0.05, loss='ls', max_depth=8,\n                          max_features='sqrt', max_leaf_nodes=None,\n                          min_impurity_decrease=0.001, min_impurity_split=None,\n                          min_samples_leaf=3, min_samples_split=10,\n                          min_weight_fraction_leaf=0.0, n_estimators=260,\n                          n_iter_no_change=None, presort='deprecated',\n                          random_state=153, subsample=1.0, tol=0.0001,\n                          validation_fraction=0.1, verbose=0, warm_start=False)\n\n\n\npred_holdouts = predict_model(dt)\npred_holdouts.head()\n\n\n                    Model        MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        Decision Tree Regressor\n                        330931.5556\n                        395886787646.2778\n                        629195.3494\n                        0.3277\n                        0.4581\n                        0.4602\n            \n    \n\n\n\n\n\n\n  \n    \n      \n      general\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      it\n      province_Anhui\n      province_Beijing\n      ...\n      year_2006\n      year_2007\n      reg_East China\n      reg_North China\n      reg_Northeast China\n      reg_Northwest China\n      reg_South Central China\n      reg_Southwest China\n      specific\n      Label\n    \n  \n  \n    \n      0\n      123546.0\n      2011.189941\n      12812.0\n      0.0\n      0.0\n      0.000000\n      1514364.0\n      2254281.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      436189.0\n      472786.0\n    \n    \n      1\n      36670.0\n      2312.820068\n      11169.0\n      0.0\n      0.0\n      0.000000\n      1600475.0\n      3035767.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      615593.0\n      601485.0\n    \n    \n      2\n      241282.0\n      6867.700195\n      53903.0\n      0.0\n      0.0\n      0.516129\n      2823413.0\n      3586373.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      685732.0\n      681676.0\n    \n    \n      3\n      581800.0\n      25776.910156\n      1101159.0\n      0.0\n      0.0\n      0.000000\n      16753980.0\n      6357869.0\n      0.0\n      0.0\n      ...\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2121243.0\n      3860764.0\n    \n    \n      4\n      36946.0\n      445.359985\n      1743.0\n      0.0\n      0.0\n      0.000000\n      233299.0\n      736165.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      133858.0\n      107687.0\n    \n  \n\n5 rows × 49 columns\n\n\n\n\nnew_data = df.copy()\nnew_data.drop(['specific'], axis=1, inplace=True)\npredict_new = predict_model(best, data=new_data)\npredict_new.head()\n\n\n\n\n\n  \n    \n      \n      province\n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      reg\n      it\n      Label\n    \n  \n  \n    \n      4\n      Anhui\n      32100.0\n      2000\n      2902.09\n      31847\n      0.0\n      0.0\n      0.000000\n      1601508\n      East China\n      1499110\n      2.000834e+05\n    \n    \n      6\n      Anhui\n      66529.0\n      2002\n      3519.72\n      38375\n      0.0\n      0.0\n      0.000000\n      1677840\n      East China\n      2404936\n      4.365530e+05\n    \n    \n      7\n      Anhui\n      52108.0\n      2003\n      3923.11\n      36720\n      0.0\n      0.0\n      0.000000\n      1896479\n      East China\n      2815820\n      6.096731e+05\n    \n    \n      10\n      Anhui\n      279052.0\n      2006\n      6112.50\n      139354\n      0.0\n      0.0\n      0.324324\n      3434548\n      East China\n      5167300\n      1.455109e+06\n    \n    \n      11\n      Anhui\n      178705.0\n      2007\n      7360.92\n      299892\n      0.0\n      0.0\n      0.324324\n      4468640\n      East China\n      7040099\n      2.000116e+06\n    \n  \n\n\n\n\n\nsave_model(best, model_name='best-model')\n\nTransformation Pipeline and Model Succesfully Saved\n\n\n(Pipeline(memory=None,\n          steps=[('dtypes',\n                  DataTypes_Auto_infer(categorical_features=[],\n                                       display_types=True, features_todrop=[],\n                                       id_columns=[], ml_usecase='regression',\n                                       numerical_features=[], target='specific',\n                                       time_features=[])),\n                 ('imputer',\n                  Simple_Imputer(categorical_strategy='not_available',\n                                 fill_value_categorical=None,\n                                 fill_value_numerical=None,\n                                 numeric_strateg...\n                                            learning_rate=0.05, loss='ls',\n                                            max_depth=8, max_features='sqrt',\n                                            max_leaf_nodes=None,\n                                            min_impurity_decrease=0.001,\n                                            min_impurity_split=None,\n                                            min_samples_leaf=3,\n                                            min_samples_split=10,\n                                            min_weight_fraction_leaf=0.0,\n                                            n_estimators=260,\n                                            n_iter_no_change=None,\n                                            presort='deprecated',\n                                            random_state=153, subsample=1.0,\n                                            tol=0.0001, validation_fraction=0.1,\n                                            verbose=0, warm_start=False)]],\n          verbose=False),\n 'best-model.pkl')\n\n\n\nloaded_bestmodel = load_model('best-model')\nprint(loaded_bestmodel)\n\nTransformation Pipeline and Model Successfully Loaded\nPipeline(memory=None,\n         steps=[('dtypes',\n                 DataTypes_Auto_infer(categorical_features=[],\n                                      display_types=True, features_todrop=[],\n                                      id_columns=[], ml_usecase='regression',\n                                      numerical_features=[], target='specific',\n                                      time_features=[])),\n                ('imputer',\n                 Simple_Imputer(categorical_strategy='not_available',\n                                fill_value_categorical=None,\n                                fill_value_numerical=None,\n                                numeric_strateg...\n                                           learning_rate=0.05, loss='ls',\n                                           max_depth=8, max_features='sqrt',\n                                           max_leaf_nodes=None,\n                                           min_impurity_decrease=0.001,\n                                           min_impurity_split=None,\n                                           min_samples_leaf=3,\n                                           min_samples_split=10,\n                                           min_weight_fraction_leaf=0.0,\n                                           n_estimators=260,\n                                           n_iter_no_change=None,\n                                           presort='deprecated',\n                                           random_state=153, subsample=1.0,\n                                           tol=0.0001, validation_fraction=0.1,\n                                           verbose=0, warm_start=False)]],\n         verbose=False)\n\n\n\nfrom sklearn import set_config\nset_config(display='diagram')\nloaded_bestmodel[0]\n\nDataTypes_Auto_inferDataTypes_Auto_infer(categorical_features=[], display_types=True,\n                     features_todrop=[], id_columns=[], ml_usecase='regression',\n                     numerical_features=[], target='specific',\n                     time_features=[])\n\n\n\nfrom sklearn import set_config\nset_config(display='text')\n\n\nX_train = get_config('X_train')\nX_train.head()\n\n\n\n\n\n  \n    \n      \n      general\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      it\n      province_Anhui\n      province_Beijing\n      ...\n      year_2002\n      year_2003\n      year_2006\n      year_2007\n      reg_East China\n      reg_North China\n      reg_Northeast China\n      reg_Northwest China\n      reg_South Central China\n      reg_Southwest China\n    \n  \n  \n    \n      343\n      66100.0\n      2556.020020\n      8384.0\n      0.0\n      0.000000\n      0.000000\n      1807967.0\n      3388449.0\n      0.0\n      0.0\n      ...\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n    \n    \n      259\n      116000.0\n      12078.150391\n      601617.0\n      0.0\n      0.000000\n      0.000000\n      6166904.0\n      2940367.0\n      0.0\n      0.0\n      ...\n      0.0\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      190\n      655919.0\n      4056.760010\n      242000.0\n      0.0\n      0.410256\n      0.000000\n      2525301.0\n      3343228.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      319\n      50097.0\n      185.089996\n      467.0\n      0.0\n      0.000000\n      0.324324\n      70048.0\n      1333133.0\n      0.0\n      0.0\n      ...\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n    \n    \n      258\n      113000.0\n      10275.500000\n      473404.0\n      0.0\n      0.000000\n      0.000000\n      5145006.0\n      2455900.0\n      0.0\n      0.0\n      ...\n      1.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n5 rows × 47 columns\n\n\n\n\nget_config('seed')\n\n153\n\n\n\nfrom pycaret.regression import set_config\nset_config('seed', 999)\n\n\nget_config('seed')\n\n999\n\n\n\n!mlflow ui \n\n[2021-05-31 20:13:02 -0500] [56453] [INFO] Starting gunicorn 20.0.4\n[2021-05-31 20:13:02 -0500] [56453] [INFO] Listening at: http://127.0.0.1:5000 (56453)\n[2021-05-31 20:13:02 -0500] [56453] [INFO] Using worker: sync\n[2021-05-31 20:13:02 -0500] [56455] [INFO] Booting worker with pid: 56455\n^C\n[2021-05-31 20:13:35 -0500] [56453] [INFO] Handling signal: int\n[2021-05-31 20:13:35 -0500] [56455] [INFO] Worker exiting (pid: 56455)"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2021-06-05-causalml-uplift_tree_visualization.html",
    "href": "posts/2021-06-05-causalml-uplift_tree_visualization.html",
    "title": "CausalML Uplift Tree Visualization",
    "section": "",
    "text": "# Code from https://github.com/uber/causalml/tree/master/examples"
  },
  {
    "objectID": "posts/2021-06-05-causalml-uplift_tree_visualization.html#introduction",
    "href": "posts/2021-06-05-causalml-uplift_tree_visualization.html#introduction",
    "title": "CausalML Uplift Tree Visualization",
    "section": "Introduction",
    "text": "Introduction\nThis example notebooks illustrates how to visualize uplift trees for interpretation and diagnosis.\n\nSupported Models\nThese visualization functions work only for tree-based algorithms:\n\nUplift tree/random forests on KL divergence, Euclidean Distance, and Chi-Square\nUplift tree/random forests on Contextual Treatment Selection\n\nCurrently, they are NOT supporting Meta-learner algorithms\n\nS-learner\nT-learner\nX-learner\nR-learner\n\n\n\nSupported Usage\nThis notebook will show how to use visualization for:\n\nUplift Tree and Uplift Random Forest\n\nVisualize a trained uplift classification tree model\nVisualize an uplift tree in a trained uplift random forests\n\nTraining and Validation Data\n\nVisualize the validation tree: fill the trained uplift classification tree with validation (or testing) data, and show the statistics for both training data and validation data\n\nOne Treatment Group and Multiple Treatment Groups\n\nVisualize the case where there are one control group and one treatment group\nVisualize the case where there are one control group and multiple treatment groups"
  },
  {
    "objectID": "posts/2021-06-05-causalml-uplift_tree_visualization.html#step-1-load-modules",
    "href": "posts/2021-06-05-causalml-uplift_tree_visualization.html#step-1-load-modules",
    "title": "CausalML Uplift Tree Visualization",
    "section": "Step 1 Load Modules",
    "text": "Step 1 Load Modules\n\nLoad CausalML modules\n\nfrom causalml.dataset import make_uplift_classification\nfrom causalml.inference.tree import UpliftTreeClassifier, UpliftRandomForestClassifier\nfrom causalml.inference.tree import uplift_tree_string, uplift_tree_plot\n\n\n\nLoad standard modules\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\nfrom sklearn.model_selection import train_test_split"
  },
  {
    "objectID": "posts/2021-06-05-causalml-uplift_tree_visualization.html#one-control-one-treatment-for-uplift-classification-tree",
    "href": "posts/2021-06-05-causalml-uplift_tree_visualization.html#one-control-one-treatment-for-uplift-classification-tree",
    "title": "CausalML Uplift Tree Visualization",
    "section": "One Control + One Treatment for Uplift Classification Tree",
    "text": "One Control + One Treatment for Uplift Classification Tree\n\n# Data generation\ndf, x_names = make_uplift_classification()\n\n# Rename features for easy interpretation of visualization\nx_names_new = ['feature_%s'%(i) for i in range(len(x_names))]\nrename_dict = {x_names[i]:x_names_new[i] for i in range(len(x_names))}\ndf = df.rename(columns=rename_dict)\nx_names = x_names_new\n\ndf.head()\n\ndf = df[df['treatment_group_key'].isin(['control','treatment1'])]\n\n# Look at the conversion rate and sample size in each group\ndf.pivot_table(values='conversion',\n               index='treatment_group_key',\n               aggfunc=[np.mean, np.size],\n               margins=True)\n\n\n\n\n\n  \n    \n      \n      mean\n      size\n    \n    \n      \n      conversion\n      conversion\n    \n    \n      treatment_group_key\n      \n      \n    \n  \n  \n    \n      control\n      0.5110\n      1000\n    \n    \n      treatment1\n      0.5140\n      1000\n    \n    \n      All\n      0.5125\n      2000\n    \n  \n\n\n\n\n\n# Split data to training and testing samples for model validation (next section)\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=111)\n\n# Train uplift tree\nuplift_model = UpliftTreeClassifier(max_depth = 4, min_samples_leaf = 200, min_samples_treatment = 50, n_reg = 100, evaluationFunction='KL', control_name='control')\n\nuplift_model.fit(df_train[x_names].values,\n                 treatment=df_train['treatment_group_key'].values,\n                 y=df_train['conversion'].values)\n\n\n# Print uplift tree as a string\nresult = uplift_tree_string(uplift_model.fitted_uplift_tree, x_names)\n\nfeature_17 >= -0.44234212654232735?\nyes -> feature_10 >= 1.020659213325515?\n        yes -> {'treatment1': 0.606557, 'control': 0.381356}\n        no  -> {'treatment1': 0.526786, 'control': 0.507812}\nno  -> feature_9 >= 0.8142773340486678?\n        yes -> {'treatment1': 0.61, 'control': 0.459677}\n        no  -> feature_4 >= 0.280545459525536?\n                yes -> {'treatment1': 0.41433, 'control': 0.552288}\n                no  -> {'treatment1': 0.574803, 'control': 0.507042}\n\n\n\nRead the tree\n\nFirst line: node split condition\nimpurity: the value for the loss function\ntotal_sample: total sample size in this node\ngroup_sample: sample size by treatment group\nuplift score: the treatment effect between treatment and control (when there are multiple treatment groups, this is the maximum of the treatment effects)\nuplift p_value: the p_value for the treatment effect\nvalidation uplift score: when validation data is filled in the tree, this reflects the uplift score based on the - validation data. It can be compared with the uplift score (for training data) to check if there are over-fitting issue.\n\n\n# Plot uplift tree\ngraph = uplift_tree_plot(uplift_model.fitted_uplift_tree,x_names)\nImage(graph.create_png())\n\n\n\n\n\n\nVisualize Validation Tree: One Control + One Treatment for Uplift Classification Tree\nNote the validation uplift score will update.\n\n### Fill the trained tree with testing data set \n# The uplift score based on testing dataset is shown as validation uplift score in the tree nodes\nuplift_model.fill(X=df_test[x_names].values, treatment=df_test['treatment_group_key'].values, y=df_test['conversion'].values)\n\n# Plot uplift tree\ngraph = uplift_tree_plot(uplift_model.fitted_uplift_tree,x_names)\nImage(graph.create_png())\n\n\n\n\n\n\nVisualize a Tree in Random Forest\n\n# Split data to training and testing samples for model validation (next section)\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=111)\n\n# Train uplift tree\nuplift_model = UpliftRandomForestClassifier(n_estimators=5, max_depth = 5, min_samples_leaf = 200, min_samples_treatment = 50, n_reg = 100, evaluationFunction='KL', control_name='control')\n\nuplift_model.fit(df_train[x_names].values,\n                 treatment=df_train['treatment_group_key'].values,\n                 y=df_train['conversion'].values)\n\n\n# Specify a tree in the random forest (the index can be any integer from 0 to n_estimators-1)\nuplift_tree = uplift_model.uplift_forest[0]\n# Print uplift tree as a string\nresult = uplift_tree_string(uplift_tree.fitted_uplift_tree, x_names)\n\nfeature_9 >= 0.7626607142400706?\nyes -> {'treatment1': 0.621795, 'control': 0.481707}\nno  -> feature_12 >= 0.5486596851987631?\n        yes -> feature_4 >= 0.9956888137470166?\n                yes -> {'treatment1': 0.496815, 'control': 0.398773}\n                no  -> {'treatment1': 0.483871, 'control': 0.551515}\n        no  -> feature_9 >= -0.15675867904794422?\n                yes -> {'treatment1': 0.456376, 'control': 0.48538}\n                no  -> {'treatment1': 0.40625, 'control': 0.625}\n\n\n\n# Plot uplift tree\ngraph = uplift_tree_plot(uplift_tree.fitted_uplift_tree,x_names)\nImage(graph.create_png())\n\n\n\n\n\nFill the tree with validation data\n\n### Fill the trained tree with testing data set \n# The uplift score based on testing dataset is shown as validation uplift score in the tree nodes\nuplift_tree.fill(X=df_test[x_names].values, treatment=df_test['treatment_group_key'].values, y=df_test['conversion'].values)\n\n# Plot uplift tree\ngraph = uplift_tree_plot(uplift_tree.fitted_uplift_tree,x_names)\nImage(graph.create_png())"
  },
  {
    "objectID": "posts/2021-06-05-causalml-uplift_tree_visualization.html#one-control-multiple-treatments",
    "href": "posts/2021-06-05-causalml-uplift_tree_visualization.html#one-control-multiple-treatments",
    "title": "CausalML Uplift Tree Visualization",
    "section": "One Control + Multiple Treatments",
    "text": "One Control + Multiple Treatments\n\n# Data generation\ndf, x_names = make_uplift_classification()\n# Look at the conversion rate and sample size in each group\ndf.pivot_table(values='conversion',\n               index='treatment_group_key',\n               aggfunc=[np.mean, np.size],\n               margins=True)\n\n\n\n\n\n  \n    \n      \n      mean\n      size\n    \n    \n      \n      conversion\n      conversion\n    \n    \n      treatment_group_key\n      \n      \n    \n  \n  \n    \n      control\n      0.511\n      1000\n    \n    \n      treatment1\n      0.514\n      1000\n    \n    \n      treatment2\n      0.559\n      1000\n    \n    \n      treatment3\n      0.600\n      1000\n    \n    \n      All\n      0.546\n      4000\n    \n  \n\n\n\n\n\n# Split data to training and testing samples for model validation (next section)\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=111)\n\n# Train uplift tree\nuplift_model = UpliftTreeClassifier(max_depth = 3, min_samples_leaf = 200, min_samples_treatment = 50, n_reg = 100, evaluationFunction='KL', control_name='control')\n\nuplift_model.fit(df_train[x_names].values,\n                 treatment=df_train['treatment_group_key'].values,\n                 y=df_train['conversion'].values)\n\n\n# Plot uplift tree\n# The uplift score represents the best uplift score among all treatment effects\ngraph = uplift_tree_plot(uplift_model.fitted_uplift_tree,x_names)\nImage(graph.create_png())\n\n\n\n\n\nSave the Plot\n\n# Save the graph as pdf\ngraph.write_pdf(\"tbc.pdf\")\n# Save the graph as png\ngraph.write_png(\"tbc.png\")\n\nTrue"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/2021-06-05-causalml-tree-based-models.html",
    "href": "posts/2021-06-05-causalml-tree-based-models.html",
    "title": "CausalML uplift with tree-based algorithms",
    "section": "",
    "text": "# Code from https://github.com/uber/causalml/tree/master/examples"
  },
  {
    "objectID": "posts/2021-06-05-causalml-tree-based-models.html#create-a-synthetic-population",
    "href": "posts/2021-06-05-causalml-tree-based-models.html#create-a-synthetic-population",
    "title": "CausalML uplift with tree-based algorithms",
    "section": "Create a synthetic population",
    "text": "Create a synthetic population\nThe uplift curve is calculated on a synthetic population that consists of those that were in the control group and those who happened to be in the treatment group recommended by the model. We use the synthetic population to calculate the actual treatment effect within predicted treatment effect quantiles. Because the data is randomized, we have a roughly equal number of treatment and control observations in the predicted quantiles and there is no self selection to treatment groups.\n\n# If all deltas are negative, assing to control; otherwise assign to the treatment\n# with the highest delta\nbest_treatment = np.where((result < 0).all(axis=1),\n                           'control',\n                           result.idxmax(axis=1))\n\n# Create indicator variables for whether a unit happened to have the\n# recommended treatment or was in the control group\nactual_is_best = np.where(df_test['treatment_group_key'] == best_treatment, 1, 0)\nactual_is_control = np.where(df_test['treatment_group_key'] == 'control', 1, 0)\n\n\nsynthetic = (actual_is_best == 1) | (actual_is_control == 1)\nsynth = result[synthetic]"
  },
  {
    "objectID": "posts/2021-06-05-causalml-tree-based-models.html#calculate-the-observed-treatment-effect-per-predicted-treatment-effect-quantile",
    "href": "posts/2021-06-05-causalml-tree-based-models.html#calculate-the-observed-treatment-effect-per-predicted-treatment-effect-quantile",
    "title": "CausalML uplift with tree-based algorithms",
    "section": "Calculate the observed treatment effect per predicted treatment effect quantile",
    "text": "Calculate the observed treatment effect per predicted treatment effect quantile\nWe use the observed treatment effect to calculate the uplift curve, which answers the question: how much of the total cumulative uplift could we have captured by targeting a subset of the population sorted according to the predicted uplift, from highest to lowest?\nCausalML has the plot_gain() function which calculates the uplift curve given a DataFrame containing the treatment assignment, observed outcome and the predicted treatment effect.\n\nauuc_metrics = (synth.assign(is_treated = 1 - actual_is_control[synthetic],\n                             conversion = df_test.loc[synthetic, 'conversion'].values,\n                             uplift_tree = synth.max(axis=1))\n                     .drop(columns=list(uplift_model.classes_)))\n\n\nplot_gain(auuc_metrics, outcome_col='conversion', treatment_col='is_treated')"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html",
    "href": "posts/2021-06-03-model-inspection.html",
    "title": "Model Inspection",
    "section": "",
    "text": "#code adapted from https://github.com/thomasjpfan/ml-workshop-intermediate-1-of-2"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#load-the-dataset",
    "href": "posts/2021-06-03-model-inspection.html#load-the-dataset",
    "title": "Model Inspection",
    "section": "Load the dataset",
    "text": "Load the dataset\n\n# %load solutions/regression_example.py\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\n\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\n\n\nX, y = df.drop(['specific', 'Unnamed: 0'], axis = 1), df['specific']\n\n\nX = X.select_dtypes(include='number')\nX\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\n\n\n\n\n\nX.head()\n\n\n\n\n\n  \n    \n      \n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      it\n    \n  \n  \n    \n      4\n      32100.0\n      2000\n      2902.09\n      31847\n      0.0\n      0.0\n      0.000000\n      1499110\n    \n    \n      6\n      66529.0\n      2002\n      3519.72\n      38375\n      0.0\n      0.0\n      0.000000\n      2404936\n    \n    \n      7\n      52108.0\n      2003\n      3923.11\n      36720\n      0.0\n      0.0\n      0.000000\n      2815820\n    \n    \n      10\n      279052.0\n      2006\n      6112.50\n      139354\n      0.0\n      0.0\n      0.324324\n      5167300\n    \n    \n      11\n      178705.0\n      2007\n      7360.92\n      299892\n      0.0\n      0.0\n      0.324324\n      7040099\n    \n  \n\n\n\n\n\ny.head()\n\n4      195580.0\n6      434149.0\n7      619201.0\n10    1457872.0\n11    2213991.0\nName: specific, dtype: float64\n\n\n\nInsert random data for demonstration\n\nimport numpy as np\n\nX = X.assign(ran_num=np.arange(0, X.shape[0]))\n\n\n\nSplit dataset\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=42)"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#train-linear-model",
    "href": "posts/2021-06-03-model-inspection.html#train-linear-model",
    "title": "Model Inspection",
    "section": "Train linear model",
    "text": "Train linear model\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import Pipeline\n\nridge = Pipeline([\n    ('scale', StandardScaler()),\n    ('reg', Ridge())\n])\nridge.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())])StandardScalerStandardScaler()RidgeRidge()\n\n\n\nridge.score(X_train, y_train)\n\n0.8843443502191103\n\n\n\nridge.score(X_test, y_test)\n\n0.7491370703502245"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#plot-coefficients",
    "href": "posts/2021-06-03-model-inspection.html#plot-coefficients",
    "title": "Model Inspection",
    "section": "Plot coefficients",
    "text": "Plot coefficients\nCoefficients represent the relationship between a feature and the target assuming that all other features remain constant.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_linear_coef(coefs, names, ax=None, sorted=False):\n    if ax is None:\n        fig, ax = plt.subplots()\n    coefs = pd.DataFrame(\n       coefs, columns=['Coefficients'],\n       index=names\n    )\n    \n    if sorted:\n        coefs = coefs.sort_values(by='Coefficients')\n\n    coefs.plot(kind='barh', ax=ax)\n    ax.axvline(x=0, color='.5')\n    return ax\n\nplot_linear_coef(ridge['reg'].coef_, names=X_train.columns, sorted=True);"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#coefficient-variability",
    "href": "posts/2021-06-03-model-inspection.html#coefficient-variability",
    "title": "Model Inspection",
    "section": "Coefficient variability",
    "text": "Coefficient variability\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import RepeatedKFold\n\n\nridges_cv = cross_validate(\n    ridge, X_train, y_train, cv=RepeatedKFold(n_splits=5, n_repeats=5),\n    return_estimator=True)\n\n\nridges_cv\n\n{'fit_time': array([0.00852752, 0.00879049, 0.00563025, 0.00589609, 0.00541282,\n        0.00482273, 0.00472617, 0.00440693, 0.00431228, 0.00409317,\n        0.00431299, 0.003896  , 0.00612736, 0.09311175, 0.00695705,\n        0.00576901, 0.00550413, 0.00539637, 0.00509334, 0.00491738,\n        0.00479674, 0.00459194, 0.00439835, 0.00426984, 0.00396895]),\n 'score_time': array([0.00384283, 0.00219274, 0.00248122, 0.00215101, 0.00219202,\n        0.00196052, 0.00188565, 0.00182557, 0.00175428, 0.00166225,\n        0.0016768 , 0.00160766, 0.00199437, 0.00305367, 0.00251174,\n        0.00237942, 0.00228405, 0.00211263, 0.00204182, 0.00194716,\n        0.00198007, 0.00189042, 0.00182319, 0.00166941, 0.00160313]),\n 'estimator': (Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())])),\n 'test_score': array([0.78102813, 0.6260654 , 0.78600362, 0.74219093, 0.97805921,\n        0.63885999, 0.90069639, 0.86957882, 0.89608878, 0.97272764,\n        0.83761009, 0.93328631, 0.85460586, 0.47428742, 0.78822307,\n        0.8933257 , 0.80865875, 0.78604436, 0.66129305, 0.93062503,\n        0.81909785, 0.86437887, 0.65233286, 0.79389227, 0.96456357])}\n\n\n\nridge_coefs = pd.DataFrame(\n   [model['reg'].coef_ for model in ridges_cv['estimator']],\n   columns=X.columns\n)\n\n\nridge_coefs.head()\n\n\n\n\n\n  \n    \n      \n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      it\n      ran_num\n    \n  \n  \n    \n      0\n      366170.413056\n      -53199.923796\n      29027.500401\n      103838.720536\n      -38871.206032\n      21826.179010\n      18730.354007\n      432388.097936\n      15395.947631\n    \n    \n      1\n      278344.791911\n      31229.815166\n      105656.219289\n      12157.657371\n      -42445.111433\n      33836.845100\n      10283.384018\n      490405.339232\n      138.790062\n    \n    \n      2\n      370195.639332\n      4058.337195\n      180433.806468\n      -20521.007552\n      -46413.645489\n      51399.811179\n      -20177.524103\n      315977.584466\n      -43552.653962\n    \n    \n      3\n      344530.343098\n      -33517.706292\n      154739.154793\n      -33164.713671\n      -22366.886793\n      -8720.784251\n      37077.197240\n      464446.608528\n      -6371.954113\n    \n    \n      4\n      296527.214315\n      -41401.308214\n      78119.685147\n      21683.387951\n      -41251.611341\n      41388.300696\n      26437.090097\n      418431.007311\n      11901.614954\n    \n  \n\n\n\n\n\nPlotting the variability of the cofficients\n\nfig, ax = plt.subplots()\n_ = ax.boxplot(ridge_coefs, vert=False, labels=ridge_coefs.columns)\n\n\n\n\n\nfrom sklearn.linear_model import Lasso\nlasso = Pipeline([\n    ('scale', StandardScaler()),\n    ('reg', Lasso(alpha=0.06))\n])\n\n\nlasso.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('scale', StandardScaler()), ('reg', Lasso(alpha=0.06))])StandardScalerStandardScaler()LassoLasso(alpha=0.06)\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 9))\nplot_linear_coef(lasso['reg'].coef_, names=X_train.columns, sorted=True, ax=ax1);\nplot_linear_coef(ridge['reg'].coef_, names=X_train.columns, sorted=True, ax=ax2);\n\n\n\n\n\nlasso_cvs = cross_validate(\n    lasso, X_train, y_train, return_estimator=True, cv=RepeatedKFold(n_splits=5, n_repeats=5)\n)\n\n\nlasso_coefs = pd.DataFrame(\n   [model['reg'].coef_ for model in lasso_cvs['estimator']],\n   columns=X.columns\n)\n\n\nfig, ax = plt.subplots()\n_ = ax.boxplot(lasso_coefs, vert=False, labels=ridge_coefs.columns)\n\n\n\n\n\n# %load solutions/03-ex01-solutions.py\nfrom sklearn.linear_model import Lasso\n\nlasso = Pipeline([\n    ('scale', StandardScaler()),\n    ('reg', Lasso(random_state=42, alpha=0.04))\n])\nlasso.fit(X_train, y_train)\n\nlasso.score(X_test, y_test)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\nplot_linear_coef(ridge['reg'].coef_, X_train.columns, ax=ax1)\nplot_linear_coef(lasso['reg'].coef_, X_train.columns, ax=ax2)\n\n<AxesSubplot:>"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#random-forest",
    "href": "posts/2021-06-03-model-inspection.html#random-forest",
    "title": "Model Inspection",
    "section": "Random Forest",
    "text": "Random Forest\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(random_state=42)\nrf.fit(X_train, y_train)\n\nRandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\nrf.score(X_train, y_train)\n\n0.9711219647906122\n\n\n\nrf.score(X_test, y_test)\n\n0.7873539531176165\n\n\n\ndef plot_importances(importances, names, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n    indices = np.argsort(importances)\n    ax.barh(range(len(importances)), importances[indices])\n    ax.set(yticks=range(len(importances)),\n           yticklabels=np.array(names)[indices]);\n\n\nimportances = rf.feature_importances_\nplot_importances(importances, X_train.columns);\n\n\n\n\nPay attention to ran_num!"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#permutation-feature-importance",
    "href": "posts/2021-06-03-model-inspection.html#permutation-feature-importance",
    "title": "Model Inspection",
    "section": "Permutation Feature Importance",
    "text": "Permutation Feature Importance\n\nCan be used on the test data!\n\nfrom sklearn.inspection import permutation_importance\n\nrf_perm_results = permutation_importance(rf, X_test, y_test,\n                                        n_repeats=10, n_jobs=-1)\n\n\ndef plot_permutation_importance(perm_results, names, ax=None):\n    perm_sorted_idx = perm_results.importances_mean.argsort()\n    if ax is None:\n        fig, ax = plt.subplots()\n    _ = ax.boxplot(perm_results.importances[perm_sorted_idx].T, vert=False,\n                   labels=np.array(names)[perm_sorted_idx])\n    return ax\n\n\n_ = plot_permutation_importance(rf_perm_results, X_test.columns)\n\n\n\n\n\n\nLoad cancer dataset\n\n# %load solutions/classifier_example.py\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['stroke', 'id'], axis = 1), df['stroke']\n\n\nX = X.select_dtypes(include='number')\nX\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\nrf.score(X_test, y_test)\n\ny_pred = rf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7131\n           1       0.00      0.00      0.00       137\n\n    accuracy                           0.98      7268\n   macro avg       0.49      0.50      0.50      7268\nweighted avg       0.96      0.98      0.97      7268\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\nRandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\nrf.score(X_test, y_test)\n\n0.9808750687947165\n\n\n\n\nPermutation importance with random forest\n\nfrom sklearn.inspection import permutation_importance\n\nrf_result = permutation_importance(rf, X_train, y_train,\n                                   n_repeats=10, n_jobs=-1)\n\n\n\nTraining data\n\n_ = plot_permutation_importance(rf_result, X)\n\n/home/david/anaconda3/lib/python3.8/site-packages/matplotlib/text.py:1163: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  if s != self._text:\n\n\n\n\n\n\nfrom scipy.stats import spearmanr\nfrom scipy.cluster import hierarchy\n\ncorr = spearmanr(X_train).correlation\ncorr_linkage = hierarchy.ward(corr)\ncorr_linkage\n\narray([[0.        , 1.        , 1.05218646, 2.        ],\n       [2.        , 5.        , 1.20582869, 3.        ],\n       [3.        , 4.        , 1.24728653, 2.        ],\n       [6.        , 7.        , 1.38884045, 5.        ]])\n\n\n\nfrom collections import defaultdict\n\ncluster_ids = hierarchy.fcluster(corr_linkage, 1, criterion='distance')\ncluster_id_to_feature_ids = defaultdict(list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nselected_features\n\n[0, 1, 2, 3, 4]\n\n\n\n\nFeature importance with selected features\n\nfrom sklearn.inspection import permutation_importance\n\nrf_sel_result = permutation_importance(\n    rf_sel, X_test, y_test, n_repeats=10, n_jobs=-1)\n\n\nfeatures_sel = data.feature_names[selected_features]\n_ = plot_permutation_importance(rf_sel_result, features_sel)"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#partial-dependence",
    "href": "posts/2021-06-03-model-inspection.html#partial-dependence",
    "title": "Model Inspection",
    "section": "Partial Dependence",
    "text": "Partial Dependence\n\nTrain a HistGradientBostingClassifer\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier \n\n\nhist = HistGradientBoostingClassifier(random_state=0)\nhist.fit(X_train, y_train)\n\nHistGradientBoostingClassifierHistGradientBoostingClassifier(random_state=0)\n\n\n\n# %load solutions/03-ex03-solutions.py\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\n\nX, y = boston.data, boston.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=0)\n\nfrom sklearn.ensemble import GradientBoostingRegressor\ngb = GradientBoostingRegressor(random_state=0)\n\ngb.fit(X_train, y_train)\n\ngb.score(X_train, y_train)\n\nplot_importances(gb.feature_importances_, boston.feature_names)\n\ngb_perm_results = permutation_importance(gb, X_test, y_test, n_repeats=10, n_jobs=-1)\n\nplot_permutation_importance(gb_perm_results, boston.feature_names)\n\nplot_partial_dependence(gb, X_test, features=[\"LSTAT\", \"RM\", \"DIS\", \"CRIM\"],\n                        feature_names=boston.feature_names, n_cols=2)\n\nplot_partial_dependence(gb, X_test, features=[('LSTAT', 'RM')],\n                        feature_names=boston.feature_names)\n\n<sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay at 0x7f0e9d226dc0>"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html",
    "href": "posts/2021-06-04-imbalanced-data.html",
    "title": "Imbalanced data",
    "section": "",
    "text": "#code adapted from https://github.com/thomasjpfan/ml-workshop-advanced"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#load-mammography-data",
    "href": "posts/2021-06-04-imbalanced-data.html#load-mammography-data",
    "title": "Imbalanced data",
    "section": "Load Mammography Data",
    "text": "Load Mammography Data\n\n# %load solutions/classifier_example.py\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['stroke', 'id'], axis = 1), df['stroke']\n\n\nX = X.select_dtypes(include='number')\nX\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\nrf.score(X_test, y_test)\n\ny_pred = rf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7139\n           1       0.17      0.01      0.01       129\n\n    accuracy                           0.98      7268\n   macro avg       0.57      0.50      0.50      7268\nweighted avg       0.97      0.98      0.97      7268\n\n\n\n\nfrom sklearn.datasets import fetch_openml\n\n\nnp.bincount(y)\n\narray([28524,   548])"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#split-data-into-train-test-split",
    "href": "posts/2021-06-04-imbalanced-data.html#split-data-into-train-test-split",
    "title": "Imbalanced data",
    "section": "Split data into train test split",
    "text": "Split data into train test split\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, random_state=0)\n\n\nBase models\n\nLinear model\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LogisticRegression\n\n\nbase_log_reg = LogisticRegression(random_state=42)\ncv_results = cross_validate(base_log_reg,\n                            X_train, y_train, scoring=['roc_auc', 'average_precision'])\n\n\ncv_results\n\n{'fit_time': array([0.18456721, 0.18922424, 0.21849942, 0.16009235, 0.14718604]),\n 'score_time': array([0.02156591, 0.01044083, 0.00906014, 0.00876856, 0.00877857]),\n 'test_roc_auc': array([0.85926447, 0.82782335, 0.83645313, 0.82829213, 0.82486117]),\n 'test_average_precision': array([0.08657556, 0.08525184, 0.10518971, 0.0796069 , 0.08259916])}\n\n\n\nlog_reg_base_auc = cv_results['test_roc_auc'].mean()\nlog_reg_base_auc\n\n0.8353388498771366\n\n\n\nlog_reg_base_ap = cv_results['test_average_precision'].mean()\nlog_reg_base_ap\n\n0.08784463474925555\n\n\n\ndef compute_metrics(estimator):\n    cv_results = cross_validate(estimator,\n                                X_train, y_train, scoring=['roc_auc', 'average_precision'])\n    return {\n        \"auc\": cv_results[\"test_roc_auc\"].mean(),\n        \"average_precision\": cv_results[\"test_average_precision\"].mean(),\n    }\n\n\nbase_log_reg_metrics = compute_metrics(base_log_reg)\nbase_log_reg_metrics\n\n{'auc': 0.8353388498771366, 'average_precision': 0.08784463474925555}\n\n\n\n\nRandom Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nbase_rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n\n\nbase_rf_metrics = compute_metrics(base_rf)\nbase_rf_metrics\n\n{'auc': 0.7132260944197908, 'average_precision': 0.04824294956894796}\n\n\n\n\n\nImbalance-learn sampler\n\nUnder sampler\n\nnp.bincount(y_train)\n\narray([21393,   411])\n\n\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\n\nunder_sampler = RandomUnderSampler(random_state=42)\n\n\nX_train_subsample, y_train_subsample = under_sampler.fit_resample(X_train, y_train)\n\n\nX_train.shape\n\n(21804, 5)\n\n\n\nX_train_subsample.shape\n\n(822, 5)\n\n\n\nnp.bincount(y_train_subsample)\n\narray([411, 411])\n\n\n\n\nOversampling\n\nfrom imblearn.over_sampling import RandomOverSampler\n\n\nover_sampler = RandomOverSampler(random_state=42)\n\n\nX_train_subsample, y_train_subsample = over_sampler.fit_resample(X_train, y_train)\n\n\nX_train_subsample.shape\n\n(42786, 5)\n\n\n\nnp.bincount(y_train_subsample)\n\narray([21393, 21393])"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#pipelines-with-imblean",
    "href": "posts/2021-06-04-imbalanced-data.html#pipelines-with-imblean",
    "title": "Imbalanced data",
    "section": "Pipelines with imblean",
    "text": "Pipelines with imblean\n\nLinear model with under sampling\n\nfrom imblearn.pipeline import make_pipeline as make_imb_pipeline\n\n\nunder_log_reg = make_imb_pipeline(\n    RandomUnderSampler(random_state=42), LogisticRegression(random_state=42))\n\n\nbase_log_reg_metrics\n\n{'auc': 0.8353388498771366, 'average_precision': 0.08784463474925555}\n\n\n\ncompute_metrics(under_log_reg)\n\n{'auc': 0.8347615373366913, 'average_precision': 0.08779628352835236}\n\n\n\n\nRandom Forest with under sampling\n\nunder_rf = make_imb_pipeline(\n    RandomUnderSampler(random_state=42), RandomForestClassifier(random_state=42))\n\n\nbase_rf_metrics\n\n{'auc': 0.7132260944197908, 'average_precision': 0.04824294956894796}\n\n\n\ncompute_metrics(under_rf)\n\n{'auc': 0.7930356061088476, 'average_precision': 0.0634524323793531}\n\n\n\n\nLinear model with over sampling\n\nover_log_reg = make_imb_pipeline(\n    RandomOverSampler(), LogisticRegression(random_state=42))\n\n\nbase_log_reg_metrics\n\n{'auc': 0.8353388498771366, 'average_precision': 0.08784463474925555}\n\n\n\ncompute_metrics(over_log_reg)\n\n{'auc': 0.835413689744715, 'average_precision': 0.08676060348173371}"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#exercise-1",
    "href": "posts/2021-06-04-imbalanced-data.html#exercise-1",
    "title": "Imbalanced data",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nUse make_imb_pipeline with RandomOverSampler to create a pipline with random forset called over_rf.\nCompute our metrics using compute_metrics.\n\n\n# %load solutions/02-ex01-solutions.py\nover_rf = make_imb_pipeline(\n    RandomOverSampler(),\n    RandomForestClassifier(random_state=42, n_jobs=-1)\n)\n\nbase_rf_metrics\n\ncompute_metrics(over_rf)\n\n{'auc': 0.713025487676646, 'average_precision': 0.04324731383558929}"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#plotting-curves-for-logistic-regression",
    "href": "posts/2021-06-04-imbalanced-data.html#plotting-curves-for-logistic-regression",
    "title": "Imbalanced data",
    "section": "Plotting curves for logistic regression",
    "text": "Plotting curves for logistic regression\n\nbase_log_reg.fit(X_train, y_train)\nunder_log_reg.fit(X_train, y_train)\nover_log_reg.fit(X_train, y_train);\n\n\nbase_log_reg.score(X_test, y_test)\n\n0.9811502476609797\n\n\n\nPlotting\n\nfrom sklearn.metrics import plot_precision_recall_curve\nfrom sklearn.metrics import plot_roc_curve\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_log_reg, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(under_log_reg, X_test, y_test, ax=ax1, name=\"undersampling\")\nplot_roc_curve(over_log_reg, X_test, y_test, ax=ax1, name=\"oversampling\")\n\nplot_precision_recall_curve(base_log_reg, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(under_log_reg, X_test, y_test, ax=ax2, name=\"undersampling\")\nplot_precision_recall_curve(over_log_reg, X_test, y_test, ax=ax2, name=\"oversampling\");\n\n\n\n\n\n# %load solutions/02-ex02-solutions.py\nbase_rf.fit(X_train, y_train)\nunder_rf.fit(X_train, y_train)\nover_rf.fit(X_train, y_train);\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_rf, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(under_rf, X_test, y_test, ax=ax1, name=\"undersampling\")\nplot_roc_curve(over_rf, X_test, y_test, ax=ax1, name=\"oversampling\")\n\nplot_precision_recall_curve(base_rf, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(under_rf, X_test, y_test, ax=ax2, name=\"undersampling\")\nplot_precision_recall_curve(over_rf, X_test, y_test, ax=ax2, name=\"oversampling\");"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#class-weights",
    "href": "posts/2021-06-04-imbalanced-data.html#class-weights",
    "title": "Imbalanced data",
    "section": "Class-Weights",
    "text": "Class-Weights\n\nLinear model with class weights\n\nclass_weight_log_reg = LogisticRegression(class_weight='balanced', random_state=42)\nclass_weight_log_reg.fit(X_train, y_train)\n\nLogisticRegressionLogisticRegression(class_weight='balanced', random_state=42)\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_log_reg, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(class_weight_log_reg, X_test, y_test, ax=ax1, name=\"class-weighted\")\n\nplot_precision_recall_curve(base_log_reg, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(class_weight_log_reg, X_test, y_test, ax=ax2, name=\"class-weighted\")\n\n<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f4ef4551550>\n\n\n\n\n\n\n\nRandom forest with class weights\n\nclass_weight_rf = RandomForestClassifier(class_weight='balanced', random_state=42)\nclass_weight_rf.fit(X_train, y_train)\n\nRandomForestClassifierRandomForestClassifier(class_weight='balanced', random_state=42)\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_rf, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(class_weight_rf, X_test, y_test, ax=ax1, name=\"class-weighted\")\n\nplot_precision_recall_curve(base_rf, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(class_weight_rf, X_test, y_test, ax=ax2, name=\"class-weighted\")\n\n<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f4ef439b910>"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#ensemble-resampling",
    "href": "posts/2021-06-04-imbalanced-data.html#ensemble-resampling",
    "title": "Imbalanced data",
    "section": "Ensemble Resampling",
    "text": "Ensemble Resampling\n\nfrom imblearn.ensemble import BalancedRandomForestClassifier\n\n\nbalanced_rf = BalancedRandomForestClassifier(random_state=0)\nbalanced_rf.fit(X_train, y_train)\n\nBalancedRandomForestClassifierBalancedRandomForestClassifier(random_state=0)\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_rf, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(under_rf, X_test, y_test, ax=ax1, name=\"undersampling\")\nplot_roc_curve(over_rf, X_test, y_test, ax=ax1, name=\"oversampling\")\nplot_roc_curve(balanced_rf, X_test, y_test, ax=ax1, name=\"balanced bagging\")\n\nplot_precision_recall_curve(base_rf, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(under_rf, X_test, y_test, ax=ax2, name=\"undersampling\")\nplot_precision_recall_curve(over_rf, X_test, y_test, ax=ax2, name=\"oversampling\");\nplot_precision_recall_curve(balanced_rf, X_test, y_test, ax=ax2, name=\"balanced bagging\")\n\n<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f4ef4021fa0>"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#smote",
    "href": "posts/2021-06-04-imbalanced-data.html#smote",
    "title": "Imbalanced data",
    "section": "SMOTE",
    "text": "SMOTE\n\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n\nX_train_smote.shape\n\n(42786, 5)\n\n\n\nnp.bincount(y_train_smote)\n\narray([21393, 21393])\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 8))\nsorting = np.argsort(y_train)\n\naxes[0].set_title(\"Original\")\naxes[0].scatter(X_train.iloc[sorting, 3], X_train.iloc[sorting, 4], c=plt.cm.tab10(y_train.iloc[sorting]), alpha=.3, s=2)\n\naxes[1].set_title(\"SMOTE\")\naxes[1].scatter(X_train_smote.iloc[:, 3], X_train_smote.iloc[:, 4], c=plt.cm.tab10(y_train_smote), alpha=.1, s=2)\n\n<matplotlib.collections.PathCollection at 0x7f4ec7f8efa0>\n\n\n\n\n\n\ncv_results\n\n{'fit_time': array([0.18456721, 0.18922424, 0.21849942, 0.16009235, 0.14718604]),\n 'score_time': array([0.02156591, 0.01044083, 0.00906014, 0.00876856, 0.00877857]),\n 'test_roc_auc': array([0.85926447, 0.82782335, 0.83645313, 0.82829213, 0.82486117]),\n 'test_average_precision': array([0.08657556, 0.08525184, 0.10518971, 0.0796069 , 0.08259916])}\n\n\n\nsmote_log_reg = make_imb_pipeline(\n    SMOTE(random_state=42), LogisticRegression(random_state=42))\ncompute_metrics(smote_log_reg)\n\n{'auc': 0.7872750489175167, 'average_precision': 0.06044681160292857}\n\n\n\nbase_rf_metrics\n\n{'auc': 0.7132260944197908, 'average_precision': 0.04824294956894796}\n\n\n\nsmote_rf = make_imb_pipeline(SMOTE(random_state=42), RandomForestClassifier(random_state=42, n_jobs=-1))\ncompute_metrics(smote_rf)\n\n{'auc': 0.7160674179833459, 'average_precision': 0.03946048645139655}"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#plotting-all-the-version-of-random-forest",
    "href": "posts/2021-06-04-imbalanced-data.html#plotting-all-the-version-of-random-forest",
    "title": "Imbalanced data",
    "section": "Plotting all the version of random forest",
    "text": "Plotting all the version of random forest\n\nsmote_rf.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('smote', SMOTE(random_state=42)),\n                ('randomforestclassifier',\n                 RandomForestClassifier(n_jobs=-1, random_state=42))])SMOTESMOTE(random_state=42)RandomForestClassifierRandomForestClassifier(n_jobs=-1, random_state=42)\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_rf, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(under_rf, X_test, y_test, ax=ax1, name=\"undersampling\")\nplot_roc_curve(over_rf, X_test, y_test, ax=ax1, name=\"oversampling\")\nplot_roc_curve(balanced_rf, X_test, y_test, ax=ax1, name=\"balanced bagging\")\nplot_roc_curve(smote_rf, X_test, y_test, ax=ax1, name=\"smote\")\n\nplot_precision_recall_curve(base_rf, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(under_rf, X_test, y_test, ax=ax2, name=\"undersampling\")\nplot_precision_recall_curve(over_rf, X_test, y_test, ax=ax2, name=\"oversampling\");\nplot_precision_recall_curve(balanced_rf, X_test, y_test, ax=ax2, name=\"balanced bagging\")\nplot_precision_recall_curve(smote_rf, X_test, y_test, ax=ax2, name=\"smote\")\n\n<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f4ef4280fa0>\n\n\n\n\n\n\n# %load solutions/02-ex03-solutions.py\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nbase_hist = HistGradientBoostingClassifier(random_state=42)\nbase_hist.fit(X_train, y_train)\n\nsmote_hist = make_imb_pipeline(\n    SMOTE(), HistGradientBoostingClassifier(random_state=42))\nsmote_hist.fit(X_train, y_train)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_hist, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(smote_hist, X_test, y_test, ax=ax1, name=\"smote\")\n\nplot_precision_recall_curve(base_hist, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(smote_hist, X_test, y_test, ax=ax2, name=\"smote\")\n\n<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f4ef4577e50>"
  }
]