[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fictional",
    "section": "",
    "text": "news\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 29, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-06-19-health_data.html",
    "href": "posts/2021-06-19-health_data.html",
    "title": "Forecasting running data",
    "section": "",
    "text": "from datetime import date\nimport os \ntoday = date.today()\n\n\nfor file in os.listdir():\n    if file.endswith('.ipynb'):\n        cd=today\n        os.rename(file, f'{today}-{file}')\n\n\nimport shutil\n\nshutil.copy(\n    os.path.join('2021-06-19-health_data.ipynb'),\n    os.path.join('../git-repos/Kearney_Data_Science/_notebooks')\n)\n\n'../git-repos/Kearney_Data_Science/_notebooks/2021-06-19-health_data.ipynb'\n\n\n\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\nengine = db.create_engine('sqlite:///../../Downloads/fitbit.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\n\nsql = \"\"\"\nselect DATE(date_time) as day\n, sum(distance_miles) as distance\nfrom distance_v\ngroup by DATE(date_time)\n\"\"\"\n\ncnxn = connection\n\ndf = pd.read_sql(sql, cnxn)\n\ndf\n\n\n\n\n\n  \n    \n      \n      day\n      distance\n    \n  \n  \n    \n      0\n      2020-12-02\n      11.238989\n    \n    \n      1\n      2020-12-03\n      7.615898\n    \n    \n      2\n      2020-12-04\n      11.392033\n    \n    \n      3\n      2020-12-05\n      9.929077\n    \n    \n      4\n      2020-12-06\n      10.442889\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      186\n      2021-06-08\n      0.935723\n    \n    \n      187\n      2021-06-09\n      4.844334\n    \n    \n      188\n      2021-06-10\n      8.554417\n    \n    \n      189\n      2021-06-11\n      6.167171\n    \n    \n      190\n      2021-06-12\n      5.006263\n    \n  \n\n191 rows × 2 columns\n\n\n\n\ndf['ds'] = df.day\ndf['y'] = df.distance\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 191 entries, 0 to 190\nData columns (total 4 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   day       191 non-null    object \n 1   distance  191 non-null    float64\n 2   ds        191 non-null    object \n 3   y         191 non-null    float64\ndtypes: float64(2), object(2)\nmemory usage: 6.1+ KB\n\n\n\nimport statsmodels.api as sm\nimport pandas as pd\nfrom prophet import Prophet\n\n\nimport pandas as pd\npd.set_option('compute.use_numexpr', False)\n\nm = Prophet()\nm.fit(df)\n\nINFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n<prophet.forecaster.Prophet at 0x7f0100bb6c70>\n\n\n\nfuture = m.make_future_dataframe(periods=365)\nfuture.tail()\n\n\n\n\n\n  \n    \n      \n      ds\n    \n  \n  \n    \n      551\n      2022-06-08\n    \n    \n      552\n      2022-06-09\n    \n    \n      553\n      2022-06-10\n    \n    \n      554\n      2022-06-11\n    \n    \n      555\n      2022-06-12\n    \n  \n\n\n\n\n\nforecast = m.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n\n\n\n\n\n  \n    \n      \n      ds\n      yhat\n      yhat_lower\n      yhat_upper\n    \n  \n  \n    \n      551\n      2022-06-08\n      9.954126\n      5.626830\n      14.090458\n    \n    \n      552\n      2022-06-09\n      11.067850\n      6.415623\n      15.269029\n    \n    \n      553\n      2022-06-10\n      9.524963\n      5.164365\n      13.816646\n    \n    \n      554\n      2022-06-11\n      10.393233\n      5.943450\n      14.634128\n    \n    \n      555\n      2022-06-12\n      11.320983\n      6.836854\n      15.647303\n    \n  \n\n\n\n\n\nfig1 = m.plot(forecast)\n\n\n\n\n\nfig2 = m.plot_components(forecast)\n\n\n\n\n\n# Python\nfig1 = m.plot(forecast)\n\n\n\n\n\n# Python\nfig2 = m.plot_components(forecast)\n\n\n\n\n\n# Python\nfrom prophet.plot import plot_plotly, plot_components_plotly\n\nplot_plotly(m, forecast)\n\n\n                                                \n\n\n\n# Python\nplot_components_plotly(m, forecast)\n\n\n                                                \n\n\n\n# Model fit\nm = Prophet() #Instanticate from Prophet class. \nm.fit(df) # Fit the Prophet model.\n\n# Predict\nfuture = m.make_future_dataframe(periods=365) # Make future date data frame for the next 365 days (it gives daily because it follows the frequency in input dataframe by default).\nforecast = m.predict(future) # Predict future value.\n\n# Plot results\nfig1 = m.plot(forecast) # Plot the fit to past data and future forcast.\nfig2 = m.plot_components(forecast) # Plot breakdown of components.\nplt.show()\nforecast # Displaying various results in table format.\n\nINFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      ds\n      trend\n      yhat_lower\n      yhat_upper\n      trend_lower\n      trend_upper\n      additive_terms\n      additive_terms_lower\n      additive_terms_upper\n      weekly\n      weekly_lower\n      weekly_upper\n      multiplicative_terms\n      multiplicative_terms_lower\n      multiplicative_terms_upper\n      yhat\n    \n  \n  \n    \n      0\n      2020-12-02\n      8.826908\n      4.690634\n      11.818085\n      8.826908\n      8.826908\n      -0.380921\n      -0.380921\n      -0.380921\n      -0.380921\n      -0.380921\n      -0.380921\n      0.0\n      0.0\n      0.0\n      8.445987\n    \n    \n      1\n      2020-12-03\n      8.805304\n      5.826039\n      12.902548\n      8.805304\n      8.805304\n      0.727109\n      0.727109\n      0.727109\n      0.727109\n      0.727109\n      0.727109\n      0.0\n      0.0\n      0.0\n      9.532413\n    \n    \n      2\n      2020-12-04\n      8.783700\n      4.373529\n      11.382860\n      8.783700\n      8.783700\n      -0.821473\n      -0.821473\n      -0.821473\n      -0.821473\n      -0.821473\n      -0.821473\n      0.0\n      0.0\n      0.0\n      7.962227\n    \n    \n      3\n      2020-12-05\n      8.762096\n      5.593958\n      12.411443\n      8.762096\n      8.762096\n      0.041102\n      0.041102\n      0.041102\n      0.041102\n      0.041102\n      0.041102\n      0.0\n      0.0\n      0.0\n      8.803198\n    \n    \n      4\n      2020-12-06\n      8.740492\n      6.342656\n      13.346898\n      8.740492\n      8.740492\n      0.963158\n      0.963158\n      0.963158\n      0.963158\n      0.963158\n      0.963158\n      0.0\n      0.0\n      0.0\n      9.703650\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      551\n      2022-06-08\n      10.335047\n      5.742947\n      14.393370\n      7.579529\n      13.222686\n      -0.380921\n      -0.380921\n      -0.380921\n      -0.380921\n      -0.380921\n      -0.380921\n      0.0\n      0.0\n      0.0\n      9.954126\n    \n    \n      552\n      2022-06-09\n      10.340742\n      6.769705\n      15.861835\n      7.575976\n      13.239863\n      0.727109\n      0.727109\n      0.727109\n      0.727109\n      0.727109\n      0.727109\n      0.0\n      0.0\n      0.0\n      11.067850\n    \n    \n      553\n      2022-06-10\n      10.346436\n      4.792225\n      14.159937\n      7.573519\n      13.261637\n      -0.821473\n      -0.821473\n      -0.821473\n      -0.821473\n      -0.821473\n      -0.821473\n      0.0\n      0.0\n      0.0\n      9.524963\n    \n    \n      554\n      2022-06-11\n      10.352131\n      5.946817\n      14.833565\n      7.570061\n      13.285497\n      0.041102\n      0.041102\n      0.041102\n      0.041102\n      0.041102\n      0.041102\n      0.0\n      0.0\n      0.0\n      10.393233\n    \n    \n      555\n      2022-06-12\n      10.357825\n      7.099368\n      15.701414\n      7.563656\n      13.302798\n      0.963158\n      0.963158\n      0.963158\n      0.963158\n      0.963158\n      0.963158\n      0.0\n      0.0\n      0.0\n      11.320983\n    \n  \n\n556 rows × 16 columns\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Load test data: log-transformed daily page views for the Wikipedia page for Peyton Manning.\n\ndf['cap'] = 10 # Saturating maximum\ndf['floor'] = 7 # Saturating minimum\n\n# Model setup\nm = Prophet(growth='logistic')\nm.add_country_holidays(country_name='US') # Adding US holiday regressor\nm.fit(df) \n\n# Future data generation\nfuture = m.make_future_dataframe(periods=365*5)\nfuture['cap'] = 10 # Saturating maximum\nfuture['floor'] = 7 # Saturating minimum\n\n# Future forecast\nforecast = m.predict(future) \n\nINFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n\n\n# Visualize\nfig1 = m.plot(forecast) # Plot the fit to past data and future forcast.\nfig2 = m.plot_components(forecast) # Plot breakdown of components.\nplt.show()\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef is_nfl_season(ds):\n    date = pd.to_datetime(ds)\n    return (date.month > 8 or date.month < 2)\n\ndf['on_season'] = df['ds'].apply(is_nfl_season) #on_season dummy.\ndf['off_season'] = ~df['ds'].apply(is_nfl_season) #off_season dummy.\n\n# set user-defined seasonality and fit\nm = Prophet(weekly_seasonality=False)\nm.add_seasonality(name='weekly_on_season', period=7, fourier_order=3, condition_name='on_season')\nm.add_seasonality(name='weekly_off_season', period=7, fourier_order=3, condition_name='off_season')\nm.fit(df)\n\n# Make the same columns to future data.\nfuture = m.make_future_dataframe(periods=365*5) # Make future date data frame for the next 365 days (it gives daily because it follows the frequency in input dataframe by default).\nfuture['on_season'] = future['ds'].apply(is_nfl_season)\nfuture['off_season'] = ~future['ds'].apply(is_nfl_season)\n\n# Predict future value.\nforecast = m.predict(future)\n\nINFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n\n# Plot results\nfig1 = m.plot(forecast) # Plot the fit to past data and future forcast.\nfig2 = m.plot_components(forecast) # Plot breakdown of components.\nplt.show()\n\n\n\n\n\n\n\n\n# After getting forecast dataframe using user-defined seasonality \"on-season\"/\"off-season\" above...\n\nfrom statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n\ndf['ds'] = pd.to_datetime(df['ds'],format='%Y-%m-%d')\ndf_res = df.merge(forecast,how=\"inner\",on=\"ds\")\ndf_res['residual'] = df_res['y'] - df_res['yhat']\nplot_acf(df_res['residual'])\nplot_pacf(df_res['residual'])\nplt.show()"
  },
  {
    "objectID": "posts/2021-06-10-regression-pycaret-2.html",
    "href": "posts/2021-06-10-regression-pycaret-2.html",
    "title": "Regression using Fiscal Data with PyCaret",
    "section": "",
    "text": "# %load solutions/regression_example.py\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\n\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\n\n\nX, y = df.drop(['specific', 'Unnamed: 0'], axis = 1), df['specific']\n\n\nX = X.select_dtypes(include='number')\nX\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\n\ndf=df.drop(['Unnamed: 0'], axis = 1)\n\n\n\n\n\nfrom pycaret.regression import *\nreg1 = setup(df, target = 'specific', session_id=153, log_experiment=True, experiment_name='fiscal')\n\n\n                    Description        Value    \n                \n                        0\n                        session_id\n                        153\n            \n            \n                        1\n                        Target\n                        specific\n            \n            \n                        2\n                        Original Data\n                        (118, 12)\n            \n            \n                        3\n                        Missing Values\n                        False\n            \n            \n                        4\n                        Numeric Features\n                        8\n            \n            \n                        5\n                        Categorical Features\n                        3\n            \n            \n                        6\n                        Ordinal Features\n                        False\n            \n            \n                        7\n                        High Cardinality Features\n                        False\n            \n            \n                        8\n                        High Cardinality Method\n                        None\n            \n            \n                        9\n                        Transformed Train Set\n                        (82, 47)\n            \n            \n                        10\n                        Transformed Test Set\n                        (36, 47)\n            \n            \n                        11\n                        Shuffle Train-Test\n                        True\n            \n            \n                        12\n                        Stratify Train-Test\n                        False\n            \n            \n                        13\n                        Fold Generator\n                        KFold\n            \n            \n                        14\n                        Fold Number\n                        10\n            \n            \n                        15\n                        CPU Jobs\n                        -1\n            \n            \n                        16\n                        Use GPU\n                        False\n            \n            \n                        17\n                        Log Experiment\n                        True\n            \n            \n                        18\n                        Experiment Name\n                        fiscal\n            \n            \n                        19\n                        USI\n                        0884\n            \n            \n                        20\n                        Imputation Type\n                        simple\n            \n            \n                        21\n                        Iterative Imputation Iteration\n                        None\n            \n            \n                        22\n                        Numeric Imputer\n                        mean\n            \n            \n                        23\n                        Iterative Imputation Numeric Model\n                        None\n            \n            \n                        24\n                        Categorical Imputer\n                        constant\n            \n            \n                        25\n                        Iterative Imputation Categorical Model\n                        None\n            \n            \n                        26\n                        Unknown Categoricals Handling\n                        least_frequent\n            \n            \n                        27\n                        Normalize\n                        False\n            \n            \n                        28\n                        Normalize Method\n                        None\n            \n            \n                        29\n                        Transformation\n                        False\n            \n            \n                        30\n                        Transformation Method\n                        None\n            \n            \n                        31\n                        PCA\n                        False\n            \n            \n                        32\n                        PCA Method\n                        None\n            \n            \n                        33\n                        PCA Components\n                        None\n            \n            \n                        34\n                        Ignore Low Variance\n                        False\n            \n            \n                        35\n                        Combine Rare Levels\n                        False\n            \n            \n                        36\n                        Rare Level Threshold\n                        None\n            \n            \n                        37\n                        Numeric Binning\n                        False\n            \n            \n                        38\n                        Remove Outliers\n                        False\n            \n            \n                        39\n                        Outliers Threshold\n                        None\n            \n            \n                        40\n                        Remove Multicollinearity\n                        False\n            \n            \n                        41\n                        Multicollinearity Threshold\n                        None\n            \n            \n                        42\n                        Clustering\n                        False\n            \n            \n                        43\n                        Clustering Iteration\n                        None\n            \n            \n                        44\n                        Polynomial Features\n                        False\n            \n            \n                        45\n                        Polynomial Degree\n                        None\n            \n            \n                        46\n                        Trignometry Features\n                        False\n            \n            \n                        47\n                        Polynomial Threshold\n                        None\n            \n            \n                        48\n                        Group Features\n                        False\n            \n            \n                        49\n                        Feature Selection\n                        False\n            \n            \n                        50\n                        Feature Selection Method\n                        classic\n            \n            \n                        51\n                        Features Selection Threshold\n                        None\n            \n            \n                        52\n                        Feature Interaction\n                        False\n            \n            \n                        53\n                        Feature Ratio\n                        False\n            \n            \n                        54\n                        Interaction Threshold\n                        None\n            \n            \n                        55\n                        Transform Target\n                        False\n            \n            \n                        56\n                        Transform Target Method\n                        box-cox\n            \n    \n\n\n\nbest_model = compare_models(fold=5)\n\n\n                    Model        MAE        MSE        RMSE        R2        RMSLE        MAPE        TT (Sec)    \n                \n                        ridge\n                        Ridge Regression\n                        203104.6812\n                        70889717760.0000\n                        264074.3844\n                        0.8726\n                        0.5540\n                        0.3956\n                        0.0080\n            \n            \n                        en\n                        Elastic Net\n                        214984.3531\n                        85422610841.6000\n                        290426.9344\n                        0.8517\n                        0.4986\n                        0.4002\n                        0.0100\n            \n            \n                        br\n                        Bayesian Ridge\n                        220166.1782\n                        95589994393.9887\n                        304401.6991\n                        0.8301\n                        0.4481\n                        0.3928\n                        0.0120\n            \n            \n                        huber\n                        Huber Regressor\n                        220856.7956\n                        112309915361.3680\n                        329346.7142\n                        0.8236\n                        0.4063\n                        0.3861\n                        0.0240\n            \n            \n                        lr\n                        Linear Regression\n                        232120.2812\n                        103810244608.0000\n                        317848.8281\n                        0.8138\n                        0.5000\n                        0.4204\n                        0.4660\n            \n            \n                        et\n                        Extra Trees Regressor\n                        221900.6764\n                        127140774212.6801\n                        338961.7060\n                        0.7945\n                        0.3815\n                        0.3499\n                        0.0580\n            \n            \n                        rf\n                        Random Forest Regressor\n                        237185.4749\n                        140134962286.5481\n                        359066.7448\n                        0.7677\n                        0.3845\n                        0.3603\n                        0.0680\n            \n            \n                        gbr\n                        Gradient Boosting Regressor\n                        238720.3298\n                        145838870195.5470\n                        366741.3828\n                        0.7624\n                        0.3810\n                        0.3619\n                        0.0200\n            \n            \n                        knn\n                        K Neighbors Regressor\n                        285577.7062\n                        149621195571.2000\n                        378386.5938\n                        0.7535\n                        0.4782\n                        0.4564\n                        0.0080\n            \n            \n                        omp\n                        Orthogonal Matching Pursuit\n                        238278.1124\n                        126779634746.9780\n                        340431.6364\n                        0.7507\n                        0.6785\n                        0.4087\n                        0.0060\n            \n            \n                        ada\n                        AdaBoost Regressor\n                        286133.9032\n                        178448925624.8169\n                        409351.8897\n                        0.7261\n                        0.4671\n                        0.4826\n                        0.0380\n            \n            \n                        par\n                        Passive Aggressive Regressor\n                        333654.6862\n                        255709611689.0604\n                        478365.7421\n                        0.6396\n                        0.6616\n                        0.4911\n                        0.0080\n            \n            \n                        lightgbm\n                        Light Gradient Boosting Machine\n                        333751.2407\n                        246645596801.5230\n                        489542.3762\n                        0.6196\n                        0.4881\n                        0.4594\n                        0.0140\n            \n            \n                        dt\n                        Decision Tree Regressor\n                        331466.4338\n                        251572931731.8265\n                        484401.1935\n                        0.5996\n                        0.4895\n                        0.4942\n                        0.0080\n            \n            \n                        lasso\n                        Lasso Regression\n                        472806.4594\n                        1744652831948.8000\n                        924353.6562\n                        -2.9647\n                        0.9793\n                        0.7323\n                        0.3020\n            \n            \n                        llar\n                        Lasso Least Angle Regression\n                        557614.3428\n                        2757565135711.8994\n                        1218269.0934\n                        -4.1517\n                        0.9882\n                        0.9808\n                        0.0120\n            \n            \n                        lar\n                        Least Angle Regression\n                        523505032166121.1875\n                        8777827809541126967434359603200.0000\n                        1651376809056778.0000\n                        -21875898822041108480.0000\n                        12.5860\n                        2953087708.0914\n                        0.0120\n            \n    \n\n\n\ngbr = create_model('gbr')\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        259539.4234\n                        188010814764.0497\n                        433602.1388\n                        0.2035\n                        0.3725\n                        0.3658\n            \n            \n                        1\n                        344439.6555\n                        261157479760.8762\n                        511035.6932\n                        0.7712\n                        0.2734\n                        0.2021\n            \n            \n                        2\n                        269448.7502\n                        89799376760.5959\n                        299665.4414\n                        0.2559\n                        0.5134\n                        0.5621\n            \n            \n                        3\n                        156389.8428\n                        56101010341.3762\n                        236856.5185\n                        0.9215\n                        0.3037\n                        0.2752\n            \n            \n                        4\n                        197734.1876\n                        68770895511.2340\n                        262242.0552\n                        0.8442\n                        0.4341\n                        0.4405\n            \n            \n                        5\n                        316382.5762\n                        190021156955.1915\n                        435914.1624\n                        0.8431\n                        0.3250\n                        0.3036\n            \n            \n                        6\n                        132877.7936\n                        48011457619.3648\n                        219115.1698\n                        0.9377\n                        0.1445\n                        0.1184\n            \n            \n                        7\n                        63780.4855\n                        6638335948.5179\n                        81475.9839\n                        0.9926\n                        0.2004\n                        0.1484\n            \n            \n                        8\n                        84622.6556\n                        19489890756.1842\n                        139606.1988\n                        0.8903\n                        0.5672\n                        0.5448\n            \n            \n                        9\n                        312499.9655\n                        219320548133.8201\n                        468316.7178\n                        0.6284\n                        0.4557\n                        0.4542\n            \n            \n                        Mean\n                        213771.5336\n                        114732096655.1211\n                        308783.0080\n                        0.7288\n                        0.3590\n                        0.3415\n            \n            \n                        SD\n                        95820.4252\n                        86484686219.4249\n                        139230.5665\n                        0.2673\n                        0.1285\n                        0.1501\n            \n    \n\n\n\nimport numpy as np\ngbrs = [create_model('gbr', learning_rate=i) for i in np.arange(0.1,1,0.1)]\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        214161.7167\n                        66589303052.3318\n                        258049.0323\n                        0.7179\n                        0.5064\n                        0.3315\n            \n            \n                        1\n                        383023.4934\n                        211259341300.2875\n                        459629.5697\n                        0.8149\n                        0.3382\n                        0.2795\n            \n            \n                        2\n                        239279.0742\n                        87706228150.9347\n                        296152.3732\n                        0.2732\n                        0.5829\n                        0.5918\n            \n            \n                        3\n                        192434.2246\n                        95811714139.4744\n                        309534.6736\n                        0.8660\n                        0.4334\n                        0.4130\n            \n            \n                        4\n                        142428.6249\n                        51054538060.0787\n                        225952.5128\n                        0.8843\n                        0.2410\n                        0.1901\n            \n            \n                        5\n                        367843.9369\n                        206432485479.9056\n                        454348.4186\n                        0.8295\n                        0.5008\n                        0.4725\n            \n            \n                        6\n                        228995.4083\n                        126313136112.6237\n                        355405.5938\n                        0.8362\n                        0.2683\n                        0.2024\n            \n            \n                        7\n                        158840.0937\n                        40230541391.6350\n                        200575.5254\n                        0.9550\n                        0.4181\n                        0.2684\n            \n            \n                        8\n                        195178.1770\n                        46865758291.9661\n                        216485.0071\n                        0.7363\n                        0.5635\n                        0.6440\n            \n            \n                        9\n                        371923.4898\n                        271120545391.5568\n                        520692.3712\n                        0.5406\n                        0.5683\n                        0.5885\n            \n            \n                        Mean\n                        249410.8239\n                        120338359137.0794\n                        329682.5078\n                        0.7454\n                        0.4421\n                        0.3982\n            \n            \n                        SD\n                        86305.1580\n                        77214568547.1943\n                        107924.9888\n                        0.1907\n                        0.1184\n                        0.1603\n            \n    \n\n\n\nprint(len(gbrs))\n\n9\n\n\n\ntuned_gbr = tune_model(gbr, n_iter=50, optimize = 'RMSE')\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        154942.6734\n                        74997708663.9170\n                        273857.0953\n                        0.6823\n                        0.2835\n                        0.2430\n            \n            \n                        1\n                        356026.5185\n                        259197991105.0069\n                        509114.9095\n                        0.7729\n                        0.3057\n                        0.2048\n            \n            \n                        2\n                        196245.5979\n                        50062838730.3433\n                        223747.2653\n                        0.5852\n                        0.4460\n                        0.4092\n            \n            \n                        3\n                        130813.1472\n                        28724978265.9940\n                        169484.4484\n                        0.9598\n                        0.3879\n                        0.3551\n            \n            \n                        4\n                        150379.1478\n                        58016543407.9028\n                        240866.2355\n                        0.8686\n                        0.3318\n                        0.2935\n            \n            \n                        5\n                        360842.5596\n                        256418976969.6155\n                        506378.2943\n                        0.7883\n                        0.3802\n                        0.3776\n            \n            \n                        6\n                        101664.5162\n                        23660563887.4224\n                        153819.9073\n                        0.9693\n                        0.1738\n                        0.1305\n            \n            \n                        7\n                        121078.1094\n                        22126447635.5874\n                        148749.6139\n                        0.9753\n                        0.3012\n                        0.2768\n            \n            \n                        8\n                        204892.4698\n                        59961129586.8548\n                        244869.6175\n                        0.6626\n                        0.7065\n                        0.9038\n            \n            \n                        9\n                        247798.4343\n                        119125737964.7928\n                        345145.9662\n                        0.7982\n                        0.3572\n                        0.3649\n            \n            \n                        Mean\n                        202468.3174\n                        95229291621.7437\n                        281603.3353\n                        0.8062\n                        0.3674\n                        0.3559\n            \n            \n                        SD\n                        88121.5888\n                        85677172457.2040\n                        126209.5604\n                        0.1300\n                        0.1326\n                        0.2000\n            \n    \n\n\n\ntuned_gbr\n\nGradientBoostingRegressorGradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n                          init=None, learning_rate=0.05, loss='ls', max_depth=8,\n                          max_features='sqrt', max_leaf_nodes=None,\n                          min_impurity_decrease=0.001, min_impurity_split=None,\n                          min_samples_leaf=3, min_samples_split=10,\n                          min_weight_fraction_leaf=0.0, n_estimators=260,\n                          n_iter_no_change=None, presort='deprecated',\n                          random_state=153, subsample=1.0, tol=0.0001,\n                          validation_fraction=0.1, verbose=0, warm_start=False)\n\n\n\ndt = create_model('dt')\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        207956.4444\n                        64209396573.1111\n                        253395.7312\n                        0.7280\n                        0.2948\n                        0.3014\n            \n            \n                        1\n                        524236.3333\n                        569991781768.5555\n                        754978.0009\n                        0.5006\n                        0.4063\n                        0.3254\n            \n            \n                        2\n                        329215.7500\n                        190393519675.5000\n                        436341.0589\n                        -0.5777\n                        0.5801\n                        0.6555\n            \n            \n                        3\n                        191451.2500\n                        124860637323.7500\n                        353356.2470\n                        0.8254\n                        0.4116\n                        0.3334\n            \n            \n                        4\n                        213423.5000\n                        76306439743.2500\n                        276236.2028\n                        0.8271\n                        0.5254\n                        0.3705\n            \n            \n                        5\n                        370690.3750\n                        199965742624.8750\n                        447175.2929\n                        0.8349\n                        0.5442\n                        0.4049\n            \n            \n                        6\n                        246669.8750\n                        129638363043.8750\n                        360053.2781\n                        0.8319\n                        0.2617\n                        0.2171\n            \n            \n                        7\n                        178672.0000\n                        77945893211.5000\n                        279187.9174\n                        0.9128\n                        0.3495\n                        0.3173\n            \n            \n                        8\n                        166587.8750\n                        46127063745.6250\n                        214772.1205\n                        0.7404\n                        0.4736\n                        0.4654\n            \n            \n                        9\n                        295026.8750\n                        185238337215.6250\n                        430393.2356\n                        0.6861\n                        0.3864\n                        0.3666\n            \n            \n                        Mean\n                        272393.0278\n                        166467717492.5666\n                        380588.9085\n                        0.6310\n                        0.4234\n                        0.3757\n            \n            \n                        SD\n                        105664.3855\n                        144523328671.4254\n                        147036.7308\n                        0.4171\n                        0.1010\n                        0.1121\n            \n    \n\n\n\nbagged_dt = ensemble_model(dt, n_estimators=50)\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        196727.1578\n                        90285018684.0473\n                        300474.6556\n                        0.6175\n                        0.3087\n                        0.2890\n            \n            \n                        1\n                        400549.3422\n                        340151201313.8998\n                        583224.8291\n                        0.7020\n                        0.3095\n                        0.2039\n            \n            \n                        2\n                        225912.9050\n                        72423782552.9881\n                        269116.6709\n                        0.3999\n                        0.4933\n                        0.5057\n            \n            \n                        3\n                        118783.1250\n                        22526053317.3862\n                        150086.8193\n                        0.9685\n                        0.3121\n                        0.2808\n            \n            \n                        4\n                        202532.9275\n                        80967074782.4978\n                        284547.1398\n                        0.8166\n                        0.4177\n                        0.4139\n            \n            \n                        5\n                        341289.0375\n                        234909197221.4275\n                        484674.3208\n                        0.8060\n                        0.3879\n                        0.3623\n            \n            \n                        6\n                        141661.2425\n                        37608256142.0725\n                        193928.4820\n                        0.9512\n                        0.1464\n                        0.1355\n            \n            \n                        7\n                        126158.8400\n                        34803232314.1118\n                        186556.2444\n                        0.9611\n                        0.3018\n                        0.2442\n            \n            \n                        8\n                        183361.3550\n                        44385100050.7344\n                        210677.7161\n                        0.7502\n                        0.5438\n                        0.5929\n            \n            \n                        9\n                        260316.8175\n                        176470763137.0123\n                        420084.2334\n                        0.7010\n                        0.3999\n                        0.3821\n            \n            \n                        Mean\n                        219729.2750\n                        113452967951.6178\n                        308337.1111\n                        0.7674\n                        0.3621\n                        0.3410\n            \n            \n                        SD\n                        87376.1209\n                        99179989784.9265\n                        135577.2615\n                        0.1680\n                        0.1066\n                        0.1322\n            \n    \n\n\n\nboosted_dt = ensemble_model(dt, method = 'Boosting')\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        261162.8889\n                        114911582041.3333\n                        338986.1089\n                        0.5132\n                        0.3872\n                        0.3925\n            \n            \n                        1\n                        422328.3333\n                        333782428795.6667\n                        577739.0664\n                        0.7076\n                        0.3211\n                        0.2542\n            \n            \n                        2\n                        232284.1250\n                        77562868468.1250\n                        278501.1104\n                        0.3573\n                        0.5087\n                        0.5015\n            \n            \n                        3\n                        197047.1250\n                        112221331803.8750\n                        334994.5250\n                        0.8431\n                        0.3502\n                        0.3395\n            \n            \n                        4\n                        285119.7500\n                        161644974606.0000\n                        402050.9602\n                        0.6338\n                        0.5596\n                        0.5738\n            \n            \n                        5\n                        473330.2500\n                        599114250508.0000\n                        774024.7092\n                        0.5053\n                        0.5119\n                        0.5217\n            \n            \n                        6\n                        108483.0000\n                        20435489036.7500\n                        142952.7511\n                        0.9735\n                        0.2098\n                        0.1500\n            \n            \n                        7\n                        157960.0000\n                        69455073830.5000\n                        263543.3054\n                        0.9223\n                        0.2735\n                        0.2394\n            \n            \n                        8\n                        120478.7500\n                        23354347455.5000\n                        152821.2925\n                        0.8686\n                        0.5655\n                        0.5893\n            \n            \n                        9\n                        231595.7500\n                        117602419885.0000\n                        342932.0922\n                        0.8007\n                        0.4401\n                        0.4379\n            \n            \n                        Mean\n                        248978.9972\n                        163008476643.0750\n                        360854.5921\n                        0.7125\n                        0.4128\n                        0.4000\n            \n            \n                        SD\n                        113864.7400\n                        167985632403.4423\n                        181086.8299\n                        0.1940\n                        0.1176\n                        0.1435\n            \n    \n\n\n\nplot_model(dt)\n\n\n\n\n\nplot_model(dt, plot = 'error')\n\n\n\n\n\nplot_model(dt, plot = 'feature')\n\n\n\n\n\nevaluate_model(dt)\n\n\n\n\n\n  \n    \n      \n      Parameters\n    \n  \n  \n    \n      ccp_alpha\n      0.0\n    \n    \n      criterion\n      mse\n    \n    \n      max_depth\n      None\n    \n    \n      max_features\n      None\n    \n    \n      max_leaf_nodes\n      None\n    \n    \n      min_impurity_decrease\n      0.0\n    \n    \n      min_impurity_split\n      None\n    \n    \n      min_samples_leaf\n      1\n    \n    \n      min_samples_split\n      2\n    \n    \n      min_weight_fraction_leaf\n      0.0\n    \n    \n      presort\n      deprecated\n    \n    \n      random_state\n      153\n    \n    \n      splitter\n      best\n    \n  \n\n\n\n\n\ninterpret_model(dt)\n\n\n\n\n\ninterpret_model(dt, plot = 'correlation')\n\n\n\n\n\ninterpret_model(dt, plot = 'reason', observation = 12)\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\nbest = automl(optimize = 'MAE')\nbest\n\nGradientBoostingRegressorGradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n                          init=None, learning_rate=0.05, loss='ls', max_depth=8,\n                          max_features='sqrt', max_leaf_nodes=None,\n                          min_impurity_decrease=0.001, min_impurity_split=None,\n                          min_samples_leaf=3, min_samples_split=10,\n                          min_weight_fraction_leaf=0.0, n_estimators=260,\n                          n_iter_no_change=None, presort='deprecated',\n                          random_state=153, subsample=1.0, tol=0.0001,\n                          validation_fraction=0.1, verbose=0, warm_start=False)\n\n\n\npred_holdouts = predict_model(dt)\npred_holdouts.head()\n\n\n                    Model        MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        Decision Tree Regressor\n                        330931.5556\n                        395886787646.2778\n                        629195.3494\n                        0.3277\n                        0.4581\n                        0.4602\n            \n    \n\n\n\n\n\n\n  \n    \n      \n      general\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      it\n      province_Anhui\n      province_Beijing\n      ...\n      year_2006\n      year_2007\n      reg_East China\n      reg_North China\n      reg_Northeast China\n      reg_Northwest China\n      reg_South Central China\n      reg_Southwest China\n      specific\n      Label\n    \n  \n  \n    \n      0\n      123546.0\n      2011.189941\n      12812.0\n      0.0\n      0.0\n      0.000000\n      1514364.0\n      2254281.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      436189.0\n      472786.0\n    \n    \n      1\n      36670.0\n      2312.820068\n      11169.0\n      0.0\n      0.0\n      0.000000\n      1600475.0\n      3035767.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      615593.0\n      601485.0\n    \n    \n      2\n      241282.0\n      6867.700195\n      53903.0\n      0.0\n      0.0\n      0.516129\n      2823413.0\n      3586373.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      685732.0\n      681676.0\n    \n    \n      3\n      581800.0\n      25776.910156\n      1101159.0\n      0.0\n      0.0\n      0.000000\n      16753980.0\n      6357869.0\n      0.0\n      0.0\n      ...\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2121243.0\n      3860764.0\n    \n    \n      4\n      36946.0\n      445.359985\n      1743.0\n      0.0\n      0.0\n      0.000000\n      233299.0\n      736165.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      133858.0\n      107687.0\n    \n  \n\n5 rows × 49 columns\n\n\n\n\nnew_data = df.copy()\nnew_data.drop(['specific'], axis=1, inplace=True)\npredict_new = predict_model(best, data=new_data)\npredict_new.head()\n\n\n\n\n\n  \n    \n      \n      province\n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      reg\n      it\n      Label\n    \n  \n  \n    \n      4\n      Anhui\n      32100.0\n      2000\n      2902.09\n      31847\n      0.0\n      0.0\n      0.000000\n      1601508\n      East China\n      1499110\n      2.000834e+05\n    \n    \n      6\n      Anhui\n      66529.0\n      2002\n      3519.72\n      38375\n      0.0\n      0.0\n      0.000000\n      1677840\n      East China\n      2404936\n      4.365530e+05\n    \n    \n      7\n      Anhui\n      52108.0\n      2003\n      3923.11\n      36720\n      0.0\n      0.0\n      0.000000\n      1896479\n      East China\n      2815820\n      6.096731e+05\n    \n    \n      10\n      Anhui\n      279052.0\n      2006\n      6112.50\n      139354\n      0.0\n      0.0\n      0.324324\n      3434548\n      East China\n      5167300\n      1.455109e+06\n    \n    \n      11\n      Anhui\n      178705.0\n      2007\n      7360.92\n      299892\n      0.0\n      0.0\n      0.324324\n      4468640\n      East China\n      7040099\n      2.000116e+06\n    \n  \n\n\n\n\n\nsave_model(best, model_name='best-model')\n\nTransformation Pipeline and Model Succesfully Saved\n\n\n(Pipeline(memory=None,\n          steps=[('dtypes',\n                  DataTypes_Auto_infer(categorical_features=[],\n                                       display_types=True, features_todrop=[],\n                                       id_columns=[], ml_usecase='regression',\n                                       numerical_features=[], target='specific',\n                                       time_features=[])),\n                 ('imputer',\n                  Simple_Imputer(categorical_strategy='not_available',\n                                 fill_value_categorical=None,\n                                 fill_value_numerical=None,\n                                 numeric_strateg...\n                                            learning_rate=0.05, loss='ls',\n                                            max_depth=8, max_features='sqrt',\n                                            max_leaf_nodes=None,\n                                            min_impurity_decrease=0.001,\n                                            min_impurity_split=None,\n                                            min_samples_leaf=3,\n                                            min_samples_split=10,\n                                            min_weight_fraction_leaf=0.0,\n                                            n_estimators=260,\n                                            n_iter_no_change=None,\n                                            presort='deprecated',\n                                            random_state=153, subsample=1.0,\n                                            tol=0.0001, validation_fraction=0.1,\n                                            verbose=0, warm_start=False)]],\n          verbose=False),\n 'best-model.pkl')\n\n\n\nloaded_bestmodel = load_model('best-model')\nprint(loaded_bestmodel)\n\nTransformation Pipeline and Model Successfully Loaded\nPipeline(memory=None,\n         steps=[('dtypes',\n                 DataTypes_Auto_infer(categorical_features=[],\n                                      display_types=True, features_todrop=[],\n                                      id_columns=[], ml_usecase='regression',\n                                      numerical_features=[], target='specific',\n                                      time_features=[])),\n                ('imputer',\n                 Simple_Imputer(categorical_strategy='not_available',\n                                fill_value_categorical=None,\n                                fill_value_numerical=None,\n                                numeric_strateg...\n                                           learning_rate=0.05, loss='ls',\n                                           max_depth=8, max_features='sqrt',\n                                           max_leaf_nodes=None,\n                                           min_impurity_decrease=0.001,\n                                           min_impurity_split=None,\n                                           min_samples_leaf=3,\n                                           min_samples_split=10,\n                                           min_weight_fraction_leaf=0.0,\n                                           n_estimators=260,\n                                           n_iter_no_change=None,\n                                           presort='deprecated',\n                                           random_state=153, subsample=1.0,\n                                           tol=0.0001, validation_fraction=0.1,\n                                           verbose=0, warm_start=False)]],\n         verbose=False)\n\n\n\nfrom sklearn import set_config\nset_config(display='diagram')\nloaded_bestmodel[0]\n\nDataTypes_Auto_inferDataTypes_Auto_infer(categorical_features=[], display_types=True,\n                     features_todrop=[], id_columns=[], ml_usecase='regression',\n                     numerical_features=[], target='specific',\n                     time_features=[])\n\n\n\nfrom sklearn import set_config\nset_config(display='text')\n\n\nX_train = get_config('X_train')\nX_train.head()\n\n\n\n\n\n  \n    \n      \n      general\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      it\n      province_Anhui\n      province_Beijing\n      ...\n      year_2002\n      year_2003\n      year_2006\n      year_2007\n      reg_East China\n      reg_North China\n      reg_Northeast China\n      reg_Northwest China\n      reg_South Central China\n      reg_Southwest China\n    \n  \n  \n    \n      343\n      66100.0\n      2556.020020\n      8384.0\n      0.0\n      0.000000\n      0.000000\n      1807967.0\n      3388449.0\n      0.0\n      0.0\n      ...\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n    \n    \n      259\n      116000.0\n      12078.150391\n      601617.0\n      0.0\n      0.000000\n      0.000000\n      6166904.0\n      2940367.0\n      0.0\n      0.0\n      ...\n      0.0\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      190\n      655919.0\n      4056.760010\n      242000.0\n      0.0\n      0.410256\n      0.000000\n      2525301.0\n      3343228.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      319\n      50097.0\n      185.089996\n      467.0\n      0.0\n      0.000000\n      0.324324\n      70048.0\n      1333133.0\n      0.0\n      0.0\n      ...\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n    \n    \n      258\n      113000.0\n      10275.500000\n      473404.0\n      0.0\n      0.000000\n      0.000000\n      5145006.0\n      2455900.0\n      0.0\n      0.0\n      ...\n      1.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n5 rows × 47 columns\n\n\n\n\nget_config('seed')\n\n153\n\n\n\nfrom pycaret.regression import set_config\nset_config('seed', 999)\n\n\nget_config('seed')\n\n999\n\n\n\n!mlflow ui \n\n[2021-05-31 20:13:02 -0500] [56453] [INFO] Starting gunicorn 20.0.4\n[2021-05-31 20:13:02 -0500] [56453] [INFO] Listening at: http://127.0.0.1:5000 (56453)\n[2021-05-31 20:13:02 -0500] [56453] [INFO] Using worker: sync\n[2021-05-31 20:13:02 -0500] [56455] [INFO] Booting worker with pid: 56455\n^C\n[2021-05-31 20:13:35 -0500] [56453] [INFO] Handling signal: int\n[2021-05-31 20:13:35 -0500] [56455] [INFO] Worker exiting (pid: 56455)"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/2020-08-30-nlp with pyspark.html",
    "href": "posts/2020-08-30-nlp with pyspark.html",
    "title": "NLP with Pyspark",
    "section": "",
    "text": "This post includes code from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-08-30-nlp with pyspark.html#using-tokenizer-and-regextokenizer",
    "href": "posts/2020-08-30-nlp with pyspark.html#using-tokenizer-and-regextokenizer",
    "title": "NLP with Pyspark",
    "section": "Using Tokenizer and RegexTokenizer",
    "text": "Using Tokenizer and RegexTokenizer\n\ntokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n\nregexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n# alternatively, pattern=\"\\\\w+\", gaps(False)\n\ncountTokens = udf(lambda words: len(words), IntegerType())\n\ntokenized = tokenizer.transform(sentenceDataFrame)\ntokenized.select(\"sentence\", \"words\")\\\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n\nregexTokenized = regexTokenizer.transform(sentenceDataFrame)\nregexTokenized.select(\"sentence\", \"words\") \\\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n\n\n+-----------------------------------+------------------------------------------+------+\nsentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\nHi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\nI wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\nLogistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n+-----------------------------------+------------------------------------------+------+\n\n+-----------------------------------+------------------------------------------+------+\nsentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\nHi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\nI wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\nLogistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n+-----------------------------------+------------------------------------------+------+"
  },
  {
    "objectID": "posts/2020-08-30-nlp with pyspark.html#removing-stop-words",
    "href": "posts/2020-08-30-nlp with pyspark.html#removing-stop-words",
    "title": "NLP with Pyspark",
    "section": "Removing Stop Words",
    "text": "Removing Stop Words\n\nfrom pyspark.ml.feature import StopWordsRemover\n\nsentenceData = spark.createDataFrame([\n    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n], [\"id\", \"raw\"])\n\nremover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\nremover.transform(sentenceData).show(truncate=False)\n\n\n+---+----------------------------+--------------------+\nid |raw                         |filtered            |\n+---+----------------------------+--------------------+\n0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n+---+----------------------------+--------------------+"
  },
  {
    "objectID": "posts/2020-08-30-nlp with pyspark.html#n-grams",
    "href": "posts/2020-08-30-nlp with pyspark.html#n-grams",
    "title": "NLP with Pyspark",
    "section": "n-grams",
    "text": "n-grams\n\nfrom pyspark.ml.feature import NGram\n\nwordDataFrame = spark.createDataFrame([\n    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n], [\"id\", \"words\"])\n\nngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n\nngramDataFrame = ngram.transform(wordDataFrame)\nngramDataFrame.select(\"ngrams\").show(truncate=False)\n\n\n+------------------------------------------------------------------+\nngrams                                                            |\n+------------------------------------------------------------------+\n[Hi I, I heard, heard about, about Spark]                         |\n[I wish, wish Java, Java could, could use, use case, case classes]|\n[Logistic regression, regression models, models are, are neat]    |\n+------------------------------------------------------------------+\n\n\n\n\n\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer\n\nsentenceData = spark.createDataFrame([\n    (0.0, \"Hi I heard about Spark\"),\n    (0.0, \"I wish Java could use case classes\"),\n    (1.0, \"Logistic regression models are neat\")\n], [\"label\", \"sentence\"])\n\nsentenceData.show()\n\n\n+-----+--------------------+\nlabel|            sentence|\n+-----+--------------------+\n  0.0|Hi I heard about ...|\n  0.0|I wish Java could...|\n  1.0|Logistic regressi...|\n+-----+--------------------+\n\n\n\n\n\ntokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\nwordsData = tokenizer.transform(sentenceData)\nwordsData.show()\n\n\n+-----+--------------------+--------------------+\nlabel|            sentence|               words|\n+-----+--------------------+--------------------+\n  0.0|Hi I heard about ...|[hi, i, heard, ab...|\n  0.0|I wish Java could...|[i, wish, java, c...|\n  1.0|Logistic regressi...|[logistic, regres...|\n+-----+--------------------+--------------------+\n\n\n\n\n\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\nfeaturizedData = hashingTF.transform(wordsData)\n# alternatively, CountVectorizer can also be used to get term frequency vectors\n\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nidfModel = idf.fit(featurizedData)\nrescaledData = idfModel.transform(featurizedData)\n\nrescaledData.select(\"label\", \"features\").show()\n\n\n+-----+--------------------+\nlabel|            features|\n+-----+--------------------+\n  0.0|(20,[6,8,13,16],[...|\n  0.0|(20,[0,2,7,13,15,...|\n  1.0|(20,[3,4,6,11,19]...|\n+-----+--------------------+"
  },
  {
    "objectID": "posts/2020-08-30-nlp with pyspark.html#countvectorizer",
    "href": "posts/2020-08-30-nlp with pyspark.html#countvectorizer",
    "title": "NLP with Pyspark",
    "section": "CountVectorizer",
    "text": "CountVectorizer\n\nfrom pyspark.ml.feature import CountVectorizer\n\n# Input data: Each row is a bag of words with a ID.\ndf = spark.createDataFrame([\n    (0, \"a b c\".split(\" \")),\n    (1, \"a b b c a\".split(\" \"))\n], [\"id\", \"words\"])\n\n# fit a CountVectorizerModel from the corpus.\ncv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n\nmodel = cv.fit(df)\n\nresult = model.transform(df)\nresult.show(truncate=False)\n\n\n+---+---------------+-------------------------+\nid |words          |features                 |\n+---+---------------+-------------------------+\n0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+---+---------------+-------------------------+\n\n\n\n\n\ndf = spark.read.load(\"/FileStore/tables/SMSSpamCollection\",\n                     format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"false\")\n\n\n\n\n\n\ndf.printSchema()\n\n\nroot\n-- _c0: string (nullable = true)\n-- _c1: string (nullable = true)\n\n\n\n\n\ndata = df.withColumnRenamed('_c0','class').withColumnRenamed('_c1','text')\n\n\n\n\n\n\ndata.printSchema()\n\n\nroot\n-- class: string (nullable = true)\n-- text: string (nullable = true)"
  },
  {
    "objectID": "posts/2020-08-30-nlp with pyspark.html#clean-and-prepare-the-data",
    "href": "posts/2020-08-30-nlp with pyspark.html#clean-and-prepare-the-data",
    "title": "NLP with Pyspark",
    "section": "Clean and Prepare the Data",
    "text": "Clean and Prepare the Data\n\nfrom pyspark.sql.functions import length\n\n\n\n\n\n\ndata = data.withColumn('length',length(data['text']))\n\n\n\n\n\n\ndata.printSchema()\n\n\nroot\n-- class: string (nullable = true)\n-- text: string (nullable = true)\n-- length: integer (nullable = true)\n\n\n\n\n\n# Pretty Clear Difference\ndata.groupby('class').mean().show()\n\n\n+-----+-----------------+\nclass|      avg(length)|\n+-----+-----------------+\n  ham| 71.4545266210897|\n spam|138.6706827309237|\n+-----+-----------------+\n\n\n\n\n\nfrom pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer\n\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\nstopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\ncount_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\nidf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\nham_spam_to_num = StringIndexer(inputCol='class',outputCol='label')\n\n\n\n\n\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.linalg import Vector\n\n\n\n\n\n\nclean_up = VectorAssembler(inputCols=['tf_idf','length'],outputCol='features')\n\n\n\n\n\n\nNaive Bayes\n\nfrom pyspark.ml.classification import NaiveBayes\n\n\n\n\n\n\n# Use defaults\nnb = NaiveBayes()\n\n\n\n\n\n\n### Pipeline\n\n\nfrom pyspark.ml import Pipeline\n\n\n\n\n\n\ndata_prep_pipe = Pipeline(stages=[ham_spam_to_num,tokenizer,stopremove,count_vec,idf,clean_up])\n\n\n\n\n\n\ncleaner = data_prep_pipe.fit(data)\n\n\n\n\n\n\nclean_data = cleaner.transform(data)\n\n\n\n\n\n\n\nTraining and Evaluation\n\nclean_data = clean_data.select(['label','features'])\n\n\n\n\n\n\nclean_data.show()\n\n\n+-----+--------------------+\nlabel|            features|\n+-----+--------------------+\n  0.0|(13424,[7,11,31,6...|\n  0.0|(13424,[0,24,297,...|\n  1.0|(13424,[2,13,19,3...|\n  0.0|(13424,[0,70,80,1...|\n  0.0|(13424,[36,134,31...|\n  1.0|(13424,[10,60,139...|\n  0.0|(13424,[10,53,103...|\n  0.0|(13424,[125,184,4...|\n  1.0|(13424,[1,47,118,...|\n  1.0|(13424,[0,1,13,27...|\n  0.0|(13424,[18,43,120...|\n  1.0|(13424,[8,17,37,8...|\n  1.0|(13424,[13,30,47,...|\n  0.0|(13424,[39,96,217...|\n  0.0|(13424,[552,1697,...|\n  1.0|(13424,[30,109,11...|\n  0.0|(13424,[82,214,47...|\n  0.0|(13424,[0,2,49,13...|\n  0.0|(13424,[0,74,105,...|\n  1.0|(13424,[4,30,33,5...|\n+-----+--------------------+\nonly showing top 20 rows\n\n\n\n\n\n(training,testing) = clean_data.randomSplit([0.7,0.3])\n\n\n\n\n\n\nspam_predictor = nb.fit(training)\n\n\n\n\n\n\ndata.printSchema()\n\n\nroot\n-- class: string (nullable = true)\n-- text: string (nullable = true)\n-- length: integer (nullable = true)\n\n\n\n\n\ntest_results = spam_predictor.transform(testing)\n\n\n\n\n\n\ntest_results.show()\n\n\n+-----+--------------------+--------------------+--------------------+----------+\nlabel|            features|       rawPrediction|         probability|prediction|\n+-----+--------------------+--------------------+--------------------+----------+\n  0.0|(13424,[0,1,2,13,...|[-605.26168264963...|[1.0,7.3447866033...|       0.0|\n  0.0|(13424,[0,1,2,41,...|[-1063.2170425771...|[1.0,9.8700382552...|       0.0|\n  0.0|(13424,[0,1,3,9,1...|[-569.95657733189...|[1.0,1.4498595638...|       0.0|\n  0.0|(13424,[0,1,5,15,...|[-998.87457222776...|[1.0,5.4020023412...|       0.0|\n  0.0|(13424,[0,1,7,15,...|[-658.37986687391...|[1.0,2.6912246466...|       0.0|\n  0.0|(13424,[0,1,14,31...|[-217.18809411711...|[1.0,3.3892033063...|       0.0|\n  0.0|(13424,[0,1,14,78...|[-688.50251926938...|[1.0,8.6317783323...|       0.0|\n  0.0|(13424,[0,1,17,19...|[-809.51840544334...|[1.0,1.3686507989...|       0.0|\n  0.0|(13424,[0,1,27,35...|[-1472.6804140726...|[0.99999999999983...|       0.0|\n  0.0|(13424,[0,1,31,43...|[-341.31126583915...|[1.0,3.4983325940...|       0.0|\n  0.0|(13424,[0,1,46,17...|[-1137.4942938439...|[5.99448563047616...|       1.0|\n  0.0|(13424,[0,1,72,10...|[-704.77256939631...|[1.0,1.2592610663...|       0.0|\n  0.0|(13424,[0,1,874,1...|[-96.404593207515...|[0.99999996015865...|       0.0|\n  0.0|(13424,[0,1,874,1...|[-98.086094104500...|[0.99999996999685...|       0.0|\n  0.0|(13424,[0,2,3,4,6...|[-1289.3891411076...|[1.0,1.3408017664...|       0.0|\n  0.0|(13424,[0,2,3,5,6...|[-2561.6651406471...|[1.0,2.6887776075...|       0.0|\n  0.0|(13424,[0,2,3,5,3...|[-490.88944126371...|[1.0,9.6538338828...|       0.0|\n  0.0|(13424,[0,2,4,5,1...|[-2493.1672898653...|[1.0,9.4058507096...|       0.0|\n  0.0|(13424,[0,2,4,7,2...|[-517.23267032348...|[1.0,2.8915589432...|       0.0|\n  0.0|(13424,[0,2,4,8,2...|[-1402.5570102185...|[1.0,6.7531061115...|       0.0|\n+-----+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n\n\n\n\n## Evaluating Model Accuracy\n\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n\n\n\n\n\nacc_eval = MulticlassClassificationEvaluator()\nacc = acc_eval.evaluate(test_results)\nprint(\"Accuracy of model at predicting spam was: {}\".format(acc))\n\n\nAccuracy of model at predicting spam was: 0.9204435112848836"
  },
  {
    "objectID": "posts/2020-08-29-clustering with pyspark.html",
    "href": "posts/2020-08-29-clustering with pyspark.html",
    "title": "Clustering with Pyspark",
    "section": "",
    "text": "This post includes code from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-08-29-clustering with pyspark.html#using-the-standardscaler",
    "href": "posts/2020-08-29-clustering with pyspark.html#using-the-standardscaler",
    "title": "Clustering with Pyspark",
    "section": "Using the StandardScaler",
    "text": "Using the StandardScaler\n\nfrom pyspark.ml.feature import StandardScaler\n\n\n\n\n\n\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)"
  },
  {
    "objectID": "posts/2020-08-29-clustering with pyspark.html#fitting-the-standardscaler",
    "href": "posts/2020-08-29-clustering with pyspark.html#fitting-the-standardscaler",
    "title": "Clustering with Pyspark",
    "section": "Fitting the StandardScaler",
    "text": "Fitting the StandardScaler\n\n# Compute summary statistics by fitting the StandardScaler\nscalerModel = scaler.fit(final_df)\n\n\n\n\n\n\n# Normalize each feature to have unit standard deviation.\ncluster_final_data = scalerModel.transform(final_df)\n\n\n\n\n\n\nkmeans3 = KMeans(featuresCol='scaledFeatures',k=3)\nkmeans2 = KMeans(featuresCol='scaledFeatures',k=2)\n\n\n\n\n\n\nmodel_k3 = kmeans3.fit(cluster_final_data)\nmodel_k2 = kmeans2.fit(cluster_final_data)\n\n\n\n\n\n\nmodel_k3.transform(cluster_final_data).groupBy('prediction').count().show()\n\n\n+----------+-----+\nprediction|count|\n+----------+-----+\n         1|   15|\n         2|   86|\n         0|  259|\n+----------+-----+\n\n\n\n\n\nmodel_k2.transform(cluster_final_data).groupBy('prediction').count().show()\n\n\n+----------+-----+\nprediction|count|\n+----------+-----+\n         1|  308|\n         0|   52|\n+----------+-----+"
  }
]